"""
ä¸»å¤„ç†æµç¨‹ï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„æ–‡æ¡£å¤„ç†pipeline
"""
from typing import List, Optional, Dict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import uuid

from .document_processor import DocumentProcessor
from .llm_client import LLMClient
from .embedding_client import EmbeddingClient
from .storage import StorageManager
from .entity_processor import EntityProcessor
from .relation_processor import RelationProcessor
from .models import MemoryCache, Entity


class TemporalMemoryGraphProcessor:
    """æ—¶åºè®°å¿†å›¾è°±å¤„ç†å™¨ - ä¸»å¤„ç†æµç¨‹"""
    
    def __init__(self, storage_path: str, window_size: int = 1000, overlap: int = 200,
                 llm_api_key: Optional[str] = None, llm_model: str = "gpt-4",
                 llm_base_url: Optional[str] = None, 
                 embedding_model_path: Optional[str] = None,
                 embedding_model_name: Optional[str] = None,
                 embedding_device: str = "cpu",
                 llm_think_mode: bool = True):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            storage_path: å­˜å‚¨è·¯å¾„
            window_size: çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            llm_api_key: LLM APIå¯†é’¥
            llm_model: LLMæ¨¡å‹åç§°
            llm_base_url: LLM APIåŸºç¡€URLï¼ˆå¯è‡ªå®šä¹‰ï¼Œå¦‚æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹æœåŠ¡ï¼‰
            embedding_model_path: Embeddingæ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            embedding_model_name: Embeddingæ¨¡å‹åç§°ï¼ˆHuggingFaceæ¨¡å‹åï¼‰
            embedding_device: Embeddingè®¡ç®—è®¾å¤‡ ("cpu" æˆ– "cuda")
            llm_think_mode: LLMæ˜¯å¦å¼€å¯thinkæ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰ã€‚å¦‚æœä¸ºFalseï¼Œä¼šåœ¨promptç»“å°¾æ·»åŠ /no_think
        """
        # åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient(
            model_path=embedding_model_path,
            model_name=embedding_model_name,
            device=embedding_device
        )
        
        # ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        default_content_snippet_length = 50
        default_relation_content_snippet_length = 50
        default_max_similar_entities = 10
        
        self.storage = StorageManager(
            storage_path, 
            embedding_client=self.embedding_client,
            entity_content_snippet_length=default_content_snippet_length,
            relation_content_snippet_length=default_relation_content_snippet_length
        )
        self.document_processor = DocumentProcessor(window_size, overlap)
        self.llm_client = LLMClient(llm_api_key, llm_model, llm_base_url, 
                                   content_snippet_length=default_content_snippet_length,
                                   think_mode=llm_think_mode)
        self.entity_processor = EntityProcessor(
            self.storage, 
            self.llm_client,
            max_similar_entities=default_max_similar_entities,
            content_snippet_length=default_content_snippet_length
        )
        self.relation_processor = RelationProcessor(self.storage, self.llm_client)
        
        # ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–é…ç½®å±æ€§
        self.similarity_threshold = 0.7
        self.max_similar_entities = default_max_similar_entities
        self.content_snippet_length = default_content_snippet_length
        self.relation_content_snippet_length = default_relation_content_snippet_length
        
        # å…³ç³»æŠ½å–é…ç½®
        self.relation_extraction_max_iterations = 3
        self.relation_extraction_absolute_max_iterations = 10
        self.relation_extraction_iterative = True
        
        # å®ä½“æŠ½å–é…ç½®
        self.entity_extraction_max_iterations = 3
        self.entity_extraction_iterative = True
        self.entity_post_enhancement = False
        
        # LLMå¹¶è¡Œé…ç½®
        self.llm_threads = 1
        
        # ç¼“å­˜è®°å¿†åŠ è½½é…ç½®
        self.load_cache_memory = False
        
        # æœç´¢é˜ˆå€¼é…ç½®ï¼ˆç”¨äºä¸‰ç§ä¸åŒçš„æœç´¢æ–¹æ³•ï¼‰
        self.jaccard_search_threshold: Optional[float] = None
        self.embedding_name_search_threshold: Optional[float] = None
        self.embedding_full_search_threshold: Optional[float] = None
        
        # å½“å‰çŠ¶æ€
        self.current_memory_cache: Optional[MemoryCache] = None
    
    def process_documents(self, document_paths: List[str], verbose: bool = True,
                         similarity_threshold: Optional[float] = None,
                         max_similar_entities: Optional[int] = None,
                         content_snippet_length: Optional[int] = None,
                         relation_content_snippet_length: Optional[int] = None,
                         entity_extraction_max_iterations: Optional[int] = None,
                         relation_extraction_absolute_max_iterations: Optional[int] = None,
                         entity_extraction_iterative: Optional[bool] = None,
                         entity_post_enhancement: Optional[bool] = None,
                         relation_extraction_max_iterations: Optional[int] = None,
                         relation_extraction_iterative: Optional[bool] = None,
                         llm_threads: Optional[int] = None,
                         load_cache_memory: Optional[bool] = None,
                         jaccard_search_threshold: Optional[float] = None,
                         embedding_name_search_threshold: Optional[float] = None,
                         embedding_full_search_threshold: Optional[float] = None):
        """
        å¤„ç†å¤šä¸ªæ–‡æ¡£
        
        Args:
            document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            similarity_threshold: å®ä½“æœç´¢ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            max_similar_entities: è¯­ä¹‰å‘é‡åˆç­›åè¿”å›çš„æœ€å¤§ç›¸ä¼¼å®ä½“æ•°é‡ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            content_snippet_length: ç”¨äºç›¸ä¼¼åº¦æœç´¢çš„å®ä½“contentæˆªå–é•¿åº¦ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            relation_content_snippet_length: ç”¨äºembeddingè®¡ç®—çš„å…³ç³»contentæˆªå–é•¿åº¦ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            entity_extraction_max_iterations: å®ä½“æŠ½å–æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            relation_extraction_absolute_max_iterations: å…³ç³»æŠ½å–ç»å¯¹æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            entity_extraction_iterative: æ˜¯å¦å¯ç”¨è¿­ä»£å®ä½“æŠ½å–ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            entity_post_enhancement: æ˜¯å¦å¯ç”¨å®ä½“åéªŒå¢å¼ºï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            relation_extraction_max_iterations: å…³ç³»æŠ½å–æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            relation_extraction_iterative: æ˜¯å¦å¯ç”¨è¿­ä»£å…³ç³»æŠ½å–ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            llm_threads: LLMå¹¶è¡Œè®¿é—®çº¿ç¨‹æ•°é‡ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            load_cache_memory: æ˜¯å¦åŠ è½½ç¼“å­˜è®°å¿†ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            jaccard_search_threshold: Jaccardæœç´¢ï¼ˆname_onlyï¼‰çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨similarity_thresholdï¼‰
            embedding_name_search_threshold: Embeddingæœç´¢ï¼ˆname_onlyï¼‰çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨similarity_thresholdï¼‰
            embedding_full_search_threshold: Embeddingæœç´¢ï¼ˆname+contentï¼‰çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨similarity_thresholdï¼‰
        """
        # ä¿å­˜åŸå§‹å€¼ï¼Œä»¥ä¾¿åœ¨æ–¹æ³•ç»“æŸæ—¶æ¢å¤
        original_values = {}
        original_components = {}
        
        # å¦‚æœæä¾›äº†å‚æ•°ï¼Œä¸´æ—¶è¦†ç›–å®ä¾‹å±æ€§
        if similarity_threshold is not None:
            original_values['similarity_threshold'] = self.similarity_threshold
            self.similarity_threshold = similarity_threshold
        
        # å¤„ç†ä¸‰ç§æœç´¢æ–¹æ³•çš„ç‹¬ç«‹é˜ˆå€¼
        if jaccard_search_threshold is not None:
            original_values['jaccard_search_threshold'] = self.jaccard_search_threshold
            self.jaccard_search_threshold = jaccard_search_threshold
        if embedding_name_search_threshold is not None:
            original_values['embedding_name_search_threshold'] = self.embedding_name_search_threshold
            self.embedding_name_search_threshold = embedding_name_search_threshold
        if embedding_full_search_threshold is not None:
            original_values['embedding_full_search_threshold'] = self.embedding_full_search_threshold
            self.embedding_full_search_threshold = embedding_full_search_threshold
        
        # å…ˆæ›´æ–°å±æ€§å€¼ï¼Œç„¶åç»Ÿä¸€æ›´æ–°ç»„ä»¶
        need_update_entity_processor = False
        final_max_similar_entities = self.max_similar_entities
        final_content_snippet_length = self.content_snippet_length
        
        if max_similar_entities is not None:
            original_values['max_similar_entities'] = self.max_similar_entities
            self.max_similar_entities = max_similar_entities
            final_max_similar_entities = max_similar_entities
            need_update_entity_processor = True
        
        if content_snippet_length is not None:
            original_values['content_snippet_length'] = self.content_snippet_length
            self.content_snippet_length = content_snippet_length
            final_content_snippet_length = content_snippet_length
            # æ›´æ–° StorageManager
            if 'storage' not in original_components:
                original_components['storage'] = self.storage
            self.storage.entity_content_snippet_length = content_snippet_length
            # æ›´æ–° LLMClient
            if 'llm_client' not in original_components:
                original_components['llm_client'] = self.llm_client
            self.llm_client.content_snippet_length = content_snippet_length
            need_update_entity_processor = True
        
        # ç»Ÿä¸€æ›´æ–° EntityProcessorï¼ˆå¦‚æœéœ€è¦ï¼‰
        if need_update_entity_processor:
            if 'entity_processor' not in original_components:
                original_components['entity_processor'] = self.entity_processor
            self.entity_processor = EntityProcessor(
                self.storage,
                self.llm_client,
                max_similar_entities=final_max_similar_entities,
                content_snippet_length=final_content_snippet_length
            )
        if relation_content_snippet_length is not None:
            original_values['relation_content_snippet_length'] = self.relation_content_snippet_length
            self.relation_content_snippet_length = relation_content_snippet_length
            # æ›´æ–° StorageManager
            if 'storage' not in original_components:
                original_components['storage'] = self.storage
            self.storage.relation_content_snippet_length = relation_content_snippet_length
        if entity_extraction_max_iterations is not None:
            original_values['entity_extraction_max_iterations'] = self.entity_extraction_max_iterations
            self.entity_extraction_max_iterations = entity_extraction_max_iterations
        if relation_extraction_absolute_max_iterations is not None:
            original_values['relation_extraction_absolute_max_iterations'] = self.relation_extraction_absolute_max_iterations
            self.relation_extraction_absolute_max_iterations = relation_extraction_absolute_max_iterations
        if entity_extraction_iterative is not None:
            original_values['entity_extraction_iterative'] = self.entity_extraction_iterative
            self.entity_extraction_iterative = entity_extraction_iterative
        if entity_post_enhancement is not None:
            original_values['entity_post_enhancement'] = self.entity_post_enhancement
            self.entity_post_enhancement = entity_post_enhancement
        if relation_extraction_max_iterations is not None:
            original_values['relation_extraction_max_iterations'] = self.relation_extraction_max_iterations
            self.relation_extraction_max_iterations = relation_extraction_max_iterations
        if relation_extraction_iterative is not None:
            original_values['relation_extraction_iterative'] = self.relation_extraction_iterative
            self.relation_extraction_iterative = relation_extraction_iterative
        if llm_threads is not None:
            original_values['llm_threads'] = self.llm_threads
            self.llm_threads = llm_threads
        if load_cache_memory is not None:
            original_values['load_cache_memory'] = self.load_cache_memory
            self.load_cache_memory = load_cache_memory
        
        try:
            if verbose:
                print(f"å¼€å§‹å¤„ç† {len(document_paths)} ä¸ªæ–‡æ¡£...")
            
            # æ–­ç‚¹ç»­ä¼ ç›¸å…³å˜é‡
            resume_document_path = None
            resume_text = None
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŠ è½½æœ€æ–°çš„è®°å¿†ç¼“å­˜å¹¶æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            if self.load_cache_memory:
                if verbose:
                    print("æ­£åœ¨åŠ è½½æœ€æ–°çš„ç¼“å­˜è®°å¿†...")
                
                # è·å–æœ€æ–°ç¼“å­˜çš„å…ƒæ•°æ®ï¼ˆåŒ…å« text å’Œ document_pathï¼‰
                latest_metadata = self.storage.get_latest_memory_cache_metadata()
                
                if latest_metadata:
                    # åŠ è½½ç¼“å­˜è®°å¿†
                    self.current_memory_cache = self.storage.load_memory_cache(latest_metadata['id'])
                    
                    if self.current_memory_cache:
                        if verbose:
                            print(f"å·²åŠ è½½ç¼“å­˜è®°å¿†: {self.current_memory_cache.id} (æ—¶é—´: {self.current_memory_cache.physical_time})")
                        
                        # æå–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
                        resume_document_path = latest_metadata.get('document_path', '')
                        resume_text = latest_metadata.get('text', '')
                        
                        if verbose:
                            if resume_document_path:
                                print(f"[æ–­ç‚¹ç»­ä¼ ] ä¸Šæ¬¡å¤„ç†çš„æ–‡æ¡£: {resume_document_path}")
                            if resume_text:
                                text_preview = resume_text[:100].replace('\n', ' ')
                                print(f"[æ–­ç‚¹ç»­ä¼ ] ä¸Šæ¬¡å¤„ç†çš„æ–‡æœ¬ç‰‡æ®µ: {text_preview}...")
                else:
                    if verbose:
                        print("æœªæ‰¾åˆ°ç¼“å­˜è®°å¿†ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
                    self.current_memory_cache = None
            else:
                if verbose:
                    print("ä¸åŠ è½½ç¼“å­˜è®°å¿†ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
                self.current_memory_cache = None
            
            # éå†æ‰€æœ‰æ–‡æ¡£çš„æ»‘åŠ¨çª—å£ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
            for chunk_idx, (input_text, document_name, is_new_document, text_start_pos, text_end_pos, total_text_length, document_path) in enumerate(
                self.document_processor.process_documents(
                    document_paths,
                    resume_document_path=resume_document_path,
                    resume_text=resume_text
                )
            ):
                if verbose:
                    print(f"\nå¤„ç†çª—å£ {chunk_idx + 1} (æ–‡æ¡£: {document_name}, ä½ç½®: {text_start_pos}-{text_end_pos}/{total_text_length})")
                
                # å¤„ç†å½“å‰çª—å£
                self._process_window(input_text, document_name, is_new_document, 
                                    text_start_pos, text_end_pos, total_text_length, verbose,
                                    document_path=document_path)
        finally:
            # æ¢å¤åŸå§‹å€¼
            for key, value in original_values.items():
                setattr(self, key, value)
            # æ¢å¤åŸå§‹ç»„ä»¶
            for key, value in original_components.items():
                setattr(self, key, value)
    
    def _process_window(self, input_text: str, document_name: str, 
                       is_new_document: bool, text_start_pos: int = 0,
                       text_end_pos: int = 0, total_text_length: int = 0,
                       verbose: bool = True, document_path: str = ""):
        """
        å¤„ç†å•ä¸ªçª—å£
        
        æµç¨‹ï¼š
        1. æ›´æ–°è®°å¿†ç¼“å­˜
        2. æŠ½å–å®ä½“ï¼ˆæ¯è½®åŒ…å«å»é‡ï¼‰
        3. æŠ½å–å…³ç³»ï¼ˆæ¯è½®åŒ…å«å»é‡ï¼‰
        4. æ£€æŸ¥è¡¥å…¨å®ä½“ï¼ˆæ ¹æ®å…³ç³»ä¸­çš„ç¼ºå¤±å®ä½“ï¼‰
        5. å®ä½“å¢å¼º
        6. å¤„ç†å®ä½“ï¼ˆæœç´¢ã€å¯¹é½ã€æ›´æ–°/æ–°å»ºï¼‰
        7. å¤„ç†å…³ç³»ï¼ˆæœç´¢ã€å¯¹é½ã€æ›´æ–°/æ–°å»ºï¼‰
        
        Args:
            input_text: å½“å‰çª—å£çš„è¾“å…¥æ–‡æœ¬
            document_name: æ–‡æ¡£åç§°
            is_new_document: æ˜¯å¦æ˜¯æ–°çš„æ–‡æ¡£
            text_start_pos: å½“å‰çª—å£åœ¨æ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆå­—ç¬¦ä½ç½®ï¼‰
            text_end_pos: å½“å‰çª—å£åœ¨æ–‡æ¡£ä¸­çš„ç»“æŸä½ç½®ï¼ˆå­—ç¬¦ä½ç½®ï¼‰
            total_text_length: æ–‡æ¡£æ€»é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            document_path: æ–‡æ¡£å®Œæ•´è·¯å¾„ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"å¤„ç†çª—å£ (æ–‡æ¡£: {document_name}, ä½ç½®: {text_start_pos}-{text_end_pos}/{total_text_length})")
            print(f"è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(input_text)} å­—ç¬¦")
            print(f"{'='*60}\n")
        
        # ========== æ­¥éª¤1ï¼šæ›´æ–°è®°å¿†ç¼“å­˜ ==========
        if verbose:
            print("## æ­¥éª¤1: æ›´æ–°è®°å¿†ç¼“å­˜")
        
        new_memory_cache = self.llm_client.update_memory_cache(
            self.current_memory_cache,
            input_text,
            document_name=document_name,
            text_start_pos=text_start_pos,
            text_end_pos=text_end_pos,
            total_text_length=total_text_length
        )
        
        # ä¿å­˜æ–°çš„memory_cacheï¼ˆä¼ é€’å½“å‰å¤„ç†çš„æ–‡æœ¬å†…å®¹å’Œæ–‡æ¡£è·¯å¾„ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        self.storage.save_memory_cache(new_memory_cache, text=input_text, document_path=document_path)
        self.current_memory_cache = new_memory_cache
        
        if verbose:
            print(f"  â””â”€ ç¼“å­˜ID: {new_memory_cache.id}\n")
        
        # ========== æ­¥éª¤2ï¼šæŠ½å–å®ä½“ ==========
        if verbose:
            print("## æ­¥éª¤2: æŠ½å–å®ä½“")
        
        extracted_entities = self.llm_client.extract_entities(
            new_memory_cache,
            input_text,
            max_iterations=self.entity_extraction_max_iterations,
            enable_iterative=self.entity_extraction_iterative,
            verbose=verbose
        )
        
        if verbose:
            print(f"  â””â”€ æŠ½å–å®Œæˆ: {len(extracted_entities)} ä¸ªå®ä½“\n")
        
        # ========== æ­¥éª¤3ï¼šæŠ½å–å…³ç³» ==========
        if verbose:
            print("## æ­¥éª¤3: æŠ½å–å…³ç³»")
        
        # åŸºäºæŠ½å–çš„å®ä½“è¿›è¡Œå…³ç³»æŠ½å–
        extracted_relations = self.llm_client.extract_relations(
            new_memory_cache,
            input_text,
            extracted_entities,
            max_iterations=self.relation_extraction_max_iterations,
            absolute_max_iterations=self.relation_extraction_absolute_max_iterations,
            enable_iterative=self.relation_extraction_iterative,
            verbose=verbose
        )
        
        if verbose:
            print(f"  â””â”€ æŠ½å–å®Œæˆ: {len(extracted_relations)} ä¸ªå…³ç³»\n")
        
        # ========== æ­¥éª¤4ï¼šæ£€æŸ¥è¡¥å…¨å®ä½“ ==========
        # ç»Ÿè®¡å…³ç³»ä¸­çš„ç¼ºå¤±å®ä½“ï¼ˆä¸åœ¨å·²æŠ½å–å®ä½“ä¸­çš„ï¼‰
        existing_entity_names = set(e['name'] for e in extracted_entities)
        missing_entity_names = set()
        
        for relation in extracted_relations:
            # æ”¯æŒæ–°æ—§æ ¼å¼ï¼ˆä¸ relation_processor.py ä¿æŒä¸€è‡´ï¼‰
            entity1_name = relation.get('entity1_name') or relation.get('entity1_name', '')
            entity2_name = relation.get('entity2_name') or relation.get('entity2_name', '')
            entity1_name = entity1_name.strip() if entity1_name else ''
            entity2_name = entity2_name.strip() if entity2_name else ''
            if entity1_name and entity1_name not in existing_entity_names:
                missing_entity_names.add(entity1_name)
            if entity2_name and entity2_name not in existing_entity_names:
                missing_entity_names.add(entity2_name)
        
        if missing_entity_names:
            if verbose:
                print(f"## æ­¥éª¤4: è¡¥å…¨ç¼ºå¤±å®ä½“ ({len(missing_entity_names)} ä¸ª)")
            
            # æŠ½å–ç¼ºå¤±å®ä½“
            missing_entities_extracted = self.llm_client.extract_entities_by_names(
                new_memory_cache,
                input_text,
                list(missing_entity_names),
                verbose=verbose
            )
            
            # åˆå¹¶åˆ°å·²æŠ½å–å®ä½“åˆ—è¡¨ï¼ˆå»é‡ï¼‰
            for entity in missing_entities_extracted:
                if entity['name'] not in existing_entity_names:
                    extracted_entities.append(entity)
                    existing_entity_names.add(entity['name'])
            
            if verbose:
                print(f"  â””â”€ è¡¥å…¨å®Œæˆ: {len(missing_entities_extracted)} ä¸ªï¼Œæ€»è®¡ {len(extracted_entities)} ä¸ªå®ä½“\n")
        else:
            if verbose:
                print("## æ­¥éª¤4: è¡¥å…¨ç¼ºå¤±å®ä½“")
                print("  â””â”€ æ— ç¼ºå¤±å®ä½“ï¼Œè·³è¿‡\n")
        
        # ========== æ­¥éª¤5ï¼šå®ä½“å¢å¼º ==========
        if self.entity_post_enhancement:
            if verbose:
                print("## æ­¥éª¤5: å®ä½“å¢å¼º")
            
            # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å®ä½“å¢å¼º
            if self.llm_threads > 1 and len(extracted_entities) > 1:
                enhanced_entities = []
                with ThreadPoolExecutor(max_workers=self.llm_threads) as executor:
                    future_entity2 = {
                        executor.submit(
                            self.llm_client.enhance_entity_content,
                            new_memory_cache,
                            input_text,
                            entity
                        ): entity
                        for entity in extracted_entities
                    }
                    
                    entity_results = {}
                    for future in as_completed(future_entity2):
                        entity = future_entity2[future]
                        try:
                            enhanced_content = future.result()
                            entity_results[entity['name']] = {
                                'name': entity['name'],
                                'content': enhanced_content
                            }
                        except Exception as e:
                            if verbose:
                                print(f"      è­¦å‘Š: {entity['name']} å¢å¼ºå¤±è´¥: {e}")
                            entity_results[entity['name']] = {
                                'name': entity['name'],
                                'content': entity['content']
                            }
                    
                    for entity in extracted_entities:
                        if entity['name'] in entity_results:
                            enhanced_entities.append(entity_results[entity['name']])
                        else:
                            enhanced_entities.append({
                                'name': entity['name'],
                                'content': entity['content']
                            })
            else:
                # å•çº¿ç¨‹å¤„ç†
                enhanced_entities = []
                for entity in extracted_entities:
                    enhanced_content = self.llm_client.enhance_entity_content(
                        new_memory_cache,
                        input_text,
                        entity
                    )
                    enhanced_entities.append({
                        'name': entity['name'],
                        'content': enhanced_content
                    })
            
            extracted_entities = enhanced_entities
            
            if verbose:
                print(f"  â””â”€ å¢å¼ºå®Œæˆ: {len(extracted_entities)} ä¸ªå®ä½“\n")
        else:
            if verbose:
                print("## æ­¥éª¤5: å®ä½“å¢å¼º")
                print("  â””â”€ å·²ç¦ç”¨ï¼Œè·³è¿‡\n")
        
        # ========== æ­¥éª¤6ï¼šå¤„ç†å®ä½“ ==========
        if verbose:
            print("## æ­¥éª¤6: å¤„ç†å®ä½“ï¼ˆæœç´¢ã€å¯¹é½ã€æ›´æ–°/æ–°å»ºï¼‰")
        
        # è®°å½•åŸå§‹å®ä½“åç§°åˆ—è¡¨ï¼ˆç”¨äºåç»­å»ºç«‹æ˜ å°„ï¼‰
        original_entity_names = [e['name'] for e in extracted_entities]
        
        # ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„å…³ç³»ï¼ˆä½¿ç”¨å®ä½“åç§°ï¼‰
        # åŒ…æ‹¬ï¼šæ­¥éª¤6ä¸­å®ä½“å¤„ç†æ—¶äº§ç”Ÿçš„å…³ç³» + æ­¥éª¤3æŠ½å–çš„å…³ç³»
        all_pending_relations_by_name = []
        # å…ˆå°†æ­¥éª¤3æŠ½å–çš„å…³ç³»æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨ï¼ˆä½¿ç”¨å®ä½“åç§°ï¼‰
        if extracted_relations:
            for rel in extracted_relations:
                entity1_name = rel.get('entity1_name') or rel.get('from_entity_name', '').strip()
                entity2_name = rel.get('entity2_name') or rel.get('to_entity_name', '').strip()
                content = rel.get('content', '').strip()
                if entity1_name and entity2_name:
                    all_pending_relations_by_name.append({
                        "entity1_name": entity1_name,
                        "entity2_name": entity2_name,
                        "content": content,
                        "relation_type": "normal"  # æŠ½å–çš„å…³ç³»é»˜è®¤ä¸ºæ™®é€šå…³ç³»
                    })
        
        # ç”¨äºå­˜å‚¨å®ä½“åç§°åˆ°IDçš„æ˜ å°„ï¼ˆé€æ­¥æ„å»ºï¼‰
        entity_name_to_id_from_entities = {}
        # ç”¨äºè®°å½•å·²å¤„ç†çš„å…³ç³»ï¼ˆä½¿ç”¨å®ä½“IDå¯¹å’Œå†…å®¹å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
        processed_relations_set = set()
        
        # å®šä¹‰å›è°ƒå‡½æ•°ï¼šåœ¨æ¯ä¸ªå®ä½“å¤„ç†å®Œåï¼Œæ£€æŸ¥å¹¶å¤„ç†æ»¡è¶³æ¡ä»¶çš„å…³ç³»
        def on_entity_processed_callback(entity, current_entity_name_to_id, current_pending_relations):
            """åœ¨æ¯ä¸ªå®ä½“å¤„ç†å®Œåè°ƒç”¨ï¼Œæ£€æŸ¥å¹¶å¤„ç†æ»¡è¶³æ¡ä»¶çš„å…³ç³»"""
            nonlocal all_pending_relations_by_name, entity_name_to_id_from_entities, processed_relations_set
            
            # æ›´æ–°å…¨å±€æ˜ å°„
            entity_name_to_id_from_entities.update(current_entity_name_to_id)
            
            # æ·»åŠ æ–°çš„å…³ç³»åˆ°å¾…å¤„ç†åˆ—è¡¨ï¼ˆä»å½“å‰å®ä½“å¤„ç†ä¸­äº§ç”Ÿçš„å…³ç³»ï¼‰
            all_pending_relations_by_name.extend(current_pending_relations)
            
            # æ£€æŸ¥æ•´ä¸ªå…³ç³»é˜Ÿåˆ—ï¼šæ˜¯å¦æœ‰å…³ç³»å·²ç»æ»¡è¶³æ¡ä»¶ï¼ˆä¸¤ä¸ªå®ä½“éƒ½å·²ç»åœ¨æ˜ å°„ä¸­ï¼‰
            ready_relations = []
            remaining_relations = []
            
            for rel_info in all_pending_relations_by_name:
                entity1_name = rel_info.get("entity1_name", "")
                entity2_name = rel_info.get("entity2_name", "")
                
                entity1_id = entity_name_to_id_from_entities.get(entity1_name)
                entity2_id = entity_name_to_id_from_entities.get(entity2_name)
                
                # éªŒè¯å®ä½“IDæ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆå®ä½“å¯èƒ½å·²è¢«åˆå¹¶ï¼ŒIDå¯èƒ½å·²å¤±æ•ˆï¼‰
                # å¦‚æœIDæ— æ•ˆï¼Œå°è¯•ä»æ•°æ®åº“æŸ¥æ‰¾æ­£ç¡®çš„ID
                if entity1_id:
                    entity1_db = self.storage.get_entity_by_id(entity1_id)
                    if not entity1_db:
                        # IDæ— æ•ˆï¼Œå°è¯•é€šè¿‡åç§°æŸ¥æ‰¾æ­£ç¡®çš„å®ä½“ID
                        if entity1_name:
                            # é€šè¿‡åç§°æœç´¢å®ä½“ï¼ˆä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ï¼‰
                            similar_entities = self.storage.search_entities_by_similarity(
                                entity1_name,
                                text_mode="name_only",
                                similarity_method="embedding"
                            )
                            if similar_entities:
                                # æ‰¾åˆ°å®ä½“ï¼Œæ›´æ–°æ˜ å°„
                                correct_entity_id = similar_entities[0].entity_id
                                entity_name_to_id_from_entities[entity1_name] = correct_entity_id
                                entity1_id = correct_entity_id
                                if verbose:
                                    print(f"  â”‚  â”œâ”€ ğŸ”„ ä¿®å¤æ˜ å°„: {entity1_name} çš„IDä»æ— æ•ˆIDæ›´æ–°ä¸º {correct_entity_id}")
                            else:
                                # æ‰¾ä¸åˆ°å®ä½“ï¼Œæ¸…é™¤æ— æ•ˆID
                                entity1_id = None
                                if verbose:
                                    print(f"  â”‚  â”œâ”€ âš ï¸  è­¦å‘Š: æ— æ³•æ‰¾åˆ°å®ä½“ {entity1_name}ï¼Œæ¸…é™¤æ— æ•ˆIDæ˜ å°„")
                        else:
                            entity1_id = None
                
                if entity2_id:
                    entity2_db = self.storage.get_entity_by_id(entity2_id)
                    if not entity2_db:
                        # IDæ— æ•ˆï¼Œå°è¯•é€šè¿‡åç§°æŸ¥æ‰¾æ­£ç¡®çš„å®ä½“ID
                        if entity2_name:
                            # é€šè¿‡åç§°æœç´¢å®ä½“ï¼ˆä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ï¼‰
                            similar_entities = self.storage.search_entities_by_similarity(
                                entity2_name,
                                text_mode="name_only",
                                similarity_method="embedding"
                            )
                            if similar_entities:
                                # æ‰¾åˆ°å®ä½“ï¼Œæ›´æ–°æ˜ å°„
                                correct_entity_id = similar_entities[0].entity_id
                                entity_name_to_id_from_entities[entity2_name] = correct_entity_id
                                entity2_id = correct_entity_id
                                if verbose:
                                    print(f"  â”‚  â”œâ”€ ğŸ”„ ä¿®å¤æ˜ å°„: {entity2_name} çš„IDä»æ— æ•ˆIDæ›´æ–°ä¸º {correct_entity_id}")
                            else:
                                # æ‰¾ä¸åˆ°å®ä½“ï¼Œæ¸…é™¤æ— æ•ˆID
                                entity2_id = None
                                if verbose:
                                    print(f"  â”‚  â”œâ”€ âš ï¸  è­¦å‘Š: æ— æ³•æ‰¾åˆ°å®ä½“ {entity2_name}ï¼Œæ¸…é™¤æ— æ•ˆIDæ˜ å°„")
                        else:
                            entity2_id = None
                
                # å¦‚æœä¸¤ä¸ªå®ä½“éƒ½å·²ç»åœ¨æ˜ å°„ä¸­ï¼Œåˆ™å¯ä»¥å¤„ç†è¿™ä¸ªå…³ç³»
                if entity1_id and entity2_id and entity1_id != entity2_id:
                    ready_relations.append({
                        "entity1_id": entity1_id,
                        "entity2_id": entity2_id,
                        "entity1_name": entity1_name,
                        "entity2_name": entity2_name,
                        "content": rel_info.get("content", ""),
                        "relation_type": rel_info.get("relation_type", "normal")
                    })
                else:
                    remaining_relations.append(rel_info)
            
            # æ›´æ–°å¾…å¤„ç†å…³ç³»åˆ—è¡¨ï¼ˆç§»é™¤å·²æ»¡è¶³æ¡ä»¶çš„å…³ç³»ï¼‰
            all_pending_relations_by_name[:] = remaining_relations
            
            # å¦‚æœæœ‰æ»¡è¶³æ¡ä»¶çš„å…³ç³»ï¼Œç«‹å³å¤„ç†
            if ready_relations:
                if verbose:
                    print(f"  â”œâ”€ æ£€æµ‹åˆ° {len(ready_relations)} ä¸ªå…³ç³»å·²æ»¡è¶³æ¡ä»¶ï¼Œç«‹å³å¤„ç†...")
                
                # å»é‡ï¼šé€šè¿‡å®ä½“å¯¹å’Œå†…å®¹åˆ¤æ–­é‡å¤
                seen_relations = set()
                unique_ready_relations = []
                for rel in ready_relations:
                    entity1_id = rel.get("entity1_id")
                    entity2_id = rel.get("entity2_id")
                    content = rel.get("content", "")
                    if entity1_id and entity2_id:
                        pair_key = tuple(sorted([entity1_id, entity2_id]))
                        content_hash = hash(content.strip().lower())
                        relation_key = (pair_key, content_hash)
                        if relation_key not in seen_relations:
                            seen_relations.add(relation_key)
                            unique_ready_relations.append(rel)
                
                # å¤„ç†æ»¡è¶³æ¡ä»¶çš„å…³ç³»
                for rel_info in unique_ready_relations:
                    entity1_id = rel_info.get("entity1_id")
                    entity2_id = rel_info.get("entity2_id")
                    entity1_name = rel_info.get("entity1_name", "")
                    entity2_name = rel_info.get("entity2_name", "")
                    content = rel_info.get("content", "")
                    
                    # ç”Ÿæˆå…³ç³»å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºæ ‡è®°å·²å¤„ç†ï¼‰
                    pair_key = tuple(sorted([entity1_id, entity2_id]))
                    content_hash = hash(content.strip().lower())
                    relation_key = (pair_key, content_hash)
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
                    if relation_key in processed_relations_set:
                        # if verbose:
                        #     print(f"  â”‚  â”œâ”€ è·³è¿‡å·²å¤„ç†å…³ç³»: {entity1_name} <-> {entity2_name}")
                        continue
                    
                    # éªŒè¯å®ä½“æ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­
                    entity1_db = self.storage.get_entity_by_id(entity1_id)
                    entity2_db = self.storage.get_entity_by_id(entity2_id)
                    
                    if not entity1_db or not entity2_db:
                        # å®ä½“ä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡
                        missing_entities = []
                        if not entity1_db:
                            missing_entities.append(f"{entity1_name} (entity_id: {entity1_id})")
                        if not entity2_db:
                            missing_entities.append(f"{entity2_name} (entity_id: {entity2_id})")
                        
                        if verbose:
                            print(f"  â”‚  â”œâ”€ âš ï¸  è­¦å‘Š: è·³è¿‡å…³ç³»å¤„ç†ï¼Œå®ä½“ä¸å­˜åœ¨äºæ•°æ®åº“: {', '.join(missing_entities)}")
                            print(f"  â”‚  â”‚   å…³ç³»å†…å®¹: {content[:100]}{'...' if len(content) > 100 else ''}")
                        continue
                    
                    # ä½¿ç”¨ relation_processor åˆ›å»ºå…³ç³»
                    try:
                        relation = self.relation_processor._process_single_relation(
                            extracted_relation={
                                'entity1_name': entity1_name,
                                'entity2_name': entity2_name,
                                'content': content
                            },
                            entity1_id=entity1_id,
                            entity2_id=entity2_id,
                            memory_cache_id=new_memory_cache.id,
                            entity1_name=entity1_name,
                            entity2_name=entity2_name,
                            verbose_relation=verbose,
                            doc_name=document_name
                        )
                    except ValueError as e:
                        # æ•è·å®ä½“æœªæ‰¾åˆ°çš„é”™è¯¯ï¼Œè®°å½•è­¦å‘Šå¹¶ç»§ç»­å¤„ç†å…¶ä»–å…³ç³»
                        if verbose:
                            print(f"  â”‚  â”œâ”€ âš ï¸  è­¦å‘Š: å¤„ç†å…³ç³»æ—¶å‡ºé”™: {e}")
                            print(f"  â”‚  â”‚   å…³ç³»: {entity1_name} <-> {entity2_name}")
                            print(f"  â”‚  â”‚   å…³ç³»å†…å®¹: {content[:100]}{'...' if len(content) > 100 else ''}")
                        continue
                    
                    if relation:
                        # æ ‡è®°ä¸ºå·²å¤„ç†
                        processed_relations_set.add(relation_key)
                        if verbose:
                            print(f"  â”‚  â”œâ”€ å·²å¤„ç†å…³ç³»: {entity1_name} <-> {entity2_name}")
        
        processed_entities, pending_relations_from_entities, entity_name_to_id_from_entities_final = self.entity_processor.process_entities(
            extracted_entities,
            new_memory_cache.id,
            self.similarity_threshold,
            memory_cache=new_memory_cache,
            doc_name=document_name,
            context_text=input_text,  # ä¼ å…¥å½“å‰å¤„ç†çš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡
            extracted_relations=extracted_relations,  # ä¼ å…¥æ­¥éª¤3æŠ½å–çš„å…³ç³»ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦å·²å­˜åœ¨å…³ç³»
            jaccard_search_threshold=self.jaccard_search_threshold,
            embedding_name_search_threshold=self.embedding_name_search_threshold,
            embedding_full_search_threshold=self.embedding_full_search_threshold,
            on_entity_processed=on_entity_processed_callback
        )
        
        # åˆå¹¶æœ€ç»ˆçš„æ˜ å°„ï¼ˆå›è°ƒå‡½æ•°ä¸­å¯èƒ½å·²ç»æ›´æ–°äº†éƒ¨åˆ†æ˜ å°„ï¼‰
        entity_name_to_id_from_entities.update(entity_name_to_id_from_entities_final)
        
        # æ›´æ–°å¾…å¤„ç†å…³ç³»åˆ—è¡¨ï¼ˆä½¿ç”¨å›è°ƒå‡½æ•°ä¸­ç»´æŠ¤çš„åˆ—è¡¨ï¼‰
        pending_relations_from_entities = all_pending_relations_by_name
        
        # æŒ‰entity_idå»é‡ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆæœ¬
        unique_entities_dict = {}
        for entity in processed_entities:
            if entity.entity_id not in unique_entities_dict:
                unique_entities_dict[entity.entity_id] = entity
            else:
                if entity.physical_time > unique_entities_dict[entity.entity_id].physical_time:
                    unique_entities_dict[entity.entity_id] = entity
        
        unique_entities = list(unique_entities_dict.values())
        
        # æ„å»ºå®Œæ•´çš„å®ä½“åç§°åˆ°entity_idçš„æ˜ å°„
        # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨åŒåå®ä½“ï¼Œé¿å…è¦†ç›–
        entity_name_to_ids = {}  # name -> List[entity_id] æ”¯æŒåŒåå®ä½“
        
        # 1. é¦–å…ˆæ·»åŠ å¤„ç†åçš„å®ä½“åç§°ï¼ˆæœ€ç»ˆåç§°ï¼‰
        for entity in unique_entities:
            if entity.name not in entity_name_to_ids:
                entity_name_to_ids[entity.name] = []
            if entity.entity_id not in entity_name_to_ids[entity.name]:
                entity_name_to_ids[entity.name].append(entity.entity_id)
        
        # 2. æ·»åŠ ä»å®ä½“å¤„ç†é˜¶æ®µè¿”å›çš„æ˜ å°„ï¼ˆåŒ…æ‹¬æ–°åˆ›å»ºçš„å®ä½“ï¼‰
        for name, entity_id in entity_name_to_id_from_entities.items():
            if name not in entity_name_to_ids:
                entity_name_to_ids[name] = []
            if entity_id not in entity_name_to_ids[name]:
                entity_name_to_ids[name].append(entity_id)
        
        # 3. å»ºç«‹åŸå§‹åç§°åˆ°entity_idçš„æ˜ å°„
        # processed_entities ä¸ extracted_entities é¡ºåºä¸€è‡´ï¼Œå¯ä»¥ä¸€ä¸€å¯¹åº”
        for i, entity in enumerate(processed_entities):
            if i < len(original_entity_names):
                original_name = original_entity_names[i]
                # å°†åŸå§‹åç§°ä¹Ÿæ˜ å°„åˆ°å¯¹åº”çš„entity_id
                if original_name not in entity_name_to_ids:
                    entity_name_to_ids[original_name] = []
                if entity.entity_id not in entity_name_to_ids[original_name]:
                    entity_name_to_ids[original_name].append(entity.entity_id)
        
        # 4. æ£€æµ‹å’Œå¤„ç†åŒåå®ä½“å†²çª
        duplicate_names = {name: ids for name, ids in entity_name_to_ids.items() if len(ids) > 1}
        entity_name_to_all_ids = {}  # ä¿ç•™æ‰€æœ‰åŒåå®ä½“çš„IDåˆ—è¡¨ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        
        if duplicate_names:
            if verbose:
                print(f"    âš ï¸  å‘ç° {len(duplicate_names)} ä¸ªåŒåå®ä½“ï¼ˆä¸åŒIDï¼‰:")
                for name, ids in duplicate_names.items():
                    print(f"      - {name}: {len(ids)} ä¸ªä¸åŒçš„entity_id {ids[:3]}{'...' if len(ids) > 3 else ''}")
            
            # å¯¹äºåŒåå®ä½“ï¼Œé€‰æ‹©ç‰ˆæœ¬æ•°æœ€å¤šçš„ä½œä¸ºä¸»è¦æ˜ å°„
            # åŒæ—¶ä¿ç•™æ‰€æœ‰IDçš„æ˜ å°„ï¼Œä»¥ä¾¿åç»­å¤„ç†
            entity_name_to_id = {}
            
            for name, ids in entity_name_to_ids.items():
                if len(ids) > 1:
                    # åŒåå®ä½“ï¼šé€‰æ‹©ç‰ˆæœ¬æ•°æœ€å¤šçš„
                    version_counts = {}
                    for eid in ids:
                        count = len(self.storage.get_entity_versions(eid))
                        version_counts[eid] = count
                    
                    # é€‰æ‹©ç‰ˆæœ¬æ•°æœ€å¤šçš„å®ä½“IDä½œä¸ºä¸»è¦æ˜ å°„
                    primary_id = max(ids, key=lambda eid: version_counts.get(eid, 0))
                    entity_name_to_id[name] = primary_id
                    entity_name_to_all_ids[name] = ids
                    
                    if verbose:
                        print(f"      é€‰æ‹©ä¸»è¦å®ä½“: {name} -> {primary_id} (ç‰ˆæœ¬æ•°: {version_counts.get(primary_id, 0)})")
                        other_ids = [eid for eid in ids if eid != primary_id]
                        if other_ids:
                            print(f"        å…¶ä»–åŒåå®ä½“: {', '.join(other_ids)}")
                else:
                    # å”¯ä¸€åç§°ï¼šç›´æ¥æ˜ å°„
                    entity_name_to_id[name] = ids[0]
        else:
            # æ²¡æœ‰åŒåå®ä½“ï¼Œç›´æ¥æ„å»ºç®€å•æ˜ å°„
            entity_name_to_id = {name: ids[0] for name, ids in entity_name_to_ids.items()}
        
        # 4. ç»Ÿè®¡åˆå¹¶æƒ…å†µï¼ˆåŸå§‹åç§°ä¸æœ€ç»ˆåç§°ä¸åŒçš„ï¼‰
        merged_mappings = []
        for i, entity in enumerate(processed_entities):
            if i < len(original_entity_names):
                original_name = original_entity_names[i]
                if original_name != entity.name:
                    merged_mappings.append((original_name, entity.name, entity.entity_id))
        
        if verbose:
            print(f"  â””â”€ å¤„ç†å®Œæˆ: {len(unique_entities)} ä¸ªå”¯ä¸€å®ä½“ï¼ˆåŸå§‹ {len(original_entity_names)} ä¸ªï¼‰")
            if merged_mappings:
                print(f"     åˆå¹¶æ˜ å°„: {len(merged_mappings)} ä¸ª")
            print()
        
        # æ­¥éª¤6.3ï¼šæ›´æ–°å¾…å¤„ç†å…³ç³»ä¸­çš„å®ä½“åç§°åˆ°IDæ˜ å°„
        # å°†pending_relations_from_entitiesä¸­çš„å®ä½“åç§°è½¬æ¢ä¸ºentity_id
        updated_pending_relations = []
        for rel_info in pending_relations_from_entities:
            entity1_name = rel_info.get("entity1_name", "")
            entity2_name = rel_info.get("entity2_name", "")
            content = rel_info.get("content", "")
            relation_type = rel_info.get("relation_type", "normal")
            
            # è·å–å®ä½“IDï¼ˆå¤„ç†åŒåå®ä½“æƒ…å†µï¼‰
            entity1_id = entity_name_to_id.get(entity1_name)
            entity2_id = entity_name_to_id.get(entity2_name)
            
        
            if entity1_id and entity2_id:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå…³ç³»ï¼ˆåŒä¸€ä¸ªå®ä½“ï¼‰
                if entity1_id == entity2_id:
                    # é™é»˜è·³è¿‡è‡ªå…³ç³»
                    continue
                
                updated_pending_relations.append({
                    "entity1_id": entity1_id,
                    "entity2_id": entity2_id,
                    "entity1_name": entity1_name,
                    "entity2_name": entity2_name,
                    "content": content,
                    "relation_type": relation_type
                })
            # é™é»˜è·³è¿‡ï¼Œä¸è¾“å‡ºè­¦å‘Š
        
        # ========== æ­¥éª¤7ï¼šå¤„ç†å…³ç³» ==========
        if verbose:
            print("## æ­¥éª¤7: å¤„ç†å…³ç³»ï¼ˆæœç´¢ã€å¯¹é½ã€æ›´æ–°/æ–°å»ºï¼‰")
        
        # æ­¥éª¤7åªå¤„ç†å‰©ä½™çš„å…³ç³»ï¼ˆé‚£äº›åœ¨æ­¥éª¤6ä¸­è¿˜ä¸æ»¡è¶³æ¡ä»¶çš„å…³ç³»ï¼‰
        # æ­¥éª¤3æŠ½å–çš„å…³ç³»å·²ç»åœ¨æ­¥éª¤6å¼€å§‹æ—¶æ·»åŠ åˆ° all_pending_relations_by_name ä¸­
        # å¹¶ä¸”åœ¨æ­¥éª¤6çš„å›è°ƒå‡½æ•°ä¸­ï¼Œå·²ç»å¤„ç†äº†æ»¡è¶³æ¡ä»¶çš„å…³ç³»
        # æ‰€ä»¥è¿™é‡Œåªéœ€è¦å¤„ç† updated_pending_relationsï¼ˆæ­¥éª¤6ä¸­å‰©ä½™çš„å…³ç³»ï¼‰
        all_pending_relations = updated_pending_relations.copy()
        
        # å°†æ­¥éª¤6ä¸­å‰©ä½™çš„å…³ç³»ï¼ˆall_pending_relations_by_nameï¼‰ä¹Ÿè½¬æ¢ä¸ºIDæ ¼å¼å¹¶æ·»åŠ 
        # è¿™äº›å…³ç³»å¯èƒ½åŒ…æ‹¬æ­¥éª¤3æŠ½å–çš„å…³ç³»ï¼Œåœ¨æ­¥éª¤6ä¸­è¿˜æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„
        for rel_info in all_pending_relations_by_name:
            entity1_name = rel_info.get("entity1_name", "")
            entity2_name = rel_info.get("entity2_name", "")
            content = rel_info.get("content", "")
            relation_type = rel_info.get("relation_type", "normal")
            
            # è·å–å®ä½“IDï¼ˆå¤„ç†åŒåå®ä½“æƒ…å†µï¼‰
            entity1_id = entity_name_to_id.get(entity1_name)
            entity2_id = entity_name_to_id.get(entity2_name)
            
            # å¦‚æœå­˜åœ¨åŒåå®ä½“ï¼Œé™é»˜å¤„ç†
            if entity1_name in duplicate_names:
                # é™é»˜å¤„ç†åŒåå®ä½“ï¼Œä½¿ç”¨ä¸»è¦ID
                pass
            
            if entity2_name in duplicate_names:
                # é™é»˜å¤„ç†åŒåå®ä½“ï¼Œä½¿ç”¨ä¸»è¦ID
                pass
            
            if entity1_id and entity2_id:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå…³ç³»ï¼ˆåŒä¸€ä¸ªå®ä½“ï¼‰
                if entity1_id == entity2_id:
                    # é™é»˜è·³è¿‡è‡ªå…³ç³»
                    continue
                
                all_pending_relations.append({
                    "entity1_id": entity1_id,
                    "entity2_id": entity2_id,
                    "entity1_name": entity1_name,
                    "entity2_name": entity2_name,
                    "content": content,
                    "relation_type": relation_type
                })
            # é™é»˜è·³è¿‡ï¼Œä¸è¾“å‡ºè­¦å‘Š
        
        # å»é‡ï¼šé€šè¿‡å®ä½“å¯¹å’Œå†…å®¹åˆ¤æ–­é‡å¤
        seen_relations = set()
        unique_pending_relations = []
        for rel in all_pending_relations:
            entity1_id = rel.get("entity1_id")
            entity2_id = rel.get("entity2_id")
            content = rel.get("content", "")
            if entity1_id and entity2_id:
                pair_key = tuple(sorted([entity1_id, entity2_id]))
                content_hash = hash(content.strip().lower())
                relation_key = (pair_key, content_hash)
                if relation_key not in seen_relations:
                    seen_relations.add(relation_key)
                    unique_pending_relations.append(rel)
        
        if verbose:
            duplicate_count = len(all_pending_relations) - len(unique_pending_relations)
            if duplicate_count > 0:
                print(f"  â”œâ”€ å¾…å¤„ç†å…³ç³»: {len(all_pending_relations)} ä¸ªï¼ˆå»é‡å: {len(unique_pending_relations)} ä¸ªï¼‰")
            else:
                print(f"  â”œâ”€ å¾…å¤„ç†å…³ç³»: {len(unique_pending_relations)} ä¸ª")
        
        # å¤„ç†æ‰€æœ‰å…³ç³»
        processed_relations = []
        for rel_info in unique_pending_relations:
            entity1_id = rel_info.get("entity1_id")
            entity2_id = rel_info.get("entity2_id")
            entity1_name = rel_info.get("entity1_name", "")
            entity2_name = rel_info.get("entity2_name", "")
            content = rel_info.get("content", "")
            
            # ç”Ÿæˆå…³ç³»å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºæ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼‰
            pair_key = tuple(sorted([entity1_id, entity2_id]))
            content_hash = hash(content.strip().lower())
            relation_key = (pair_key, content_hash)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æ­¥éª¤6ä¸­å¤„ç†è¿‡
            if relation_key in processed_relations_set:
                # if verbose:
                #     print(f"    è·³è¿‡å·²å¤„ç†å…³ç³»: {entity1_name} <-> {entity2_name}")
                continue
            
            # ä½¿ç”¨ relation_processor åˆ›å»ºå…³ç³»
            relation = self.relation_processor._process_single_relation(
                extracted_relation={
                    'entity1_name': entity1_name,
                    'entity2_name': entity2_name,
                    'content': content
                },
                entity1_id=entity1_id,
                entity2_id=entity2_id,
                memory_cache_id=new_memory_cache.id,
                entity1_name=entity1_name,
                entity2_name=entity2_name,
                verbose_relation=verbose,
                doc_name=document_name
            )
            
            if relation:
                # æ ‡è®°ä¸ºå·²å¤„ç†
                processed_relations_set.add(relation_key)
                processed_relations.append(relation)
        
        all_processed_relations = processed_relations
        
        if verbose:
            print(f"  â””â”€ å¤„ç†å®Œæˆ: {len(all_processed_relations)} ä¸ªå…³ç³»\n")
            for relation in all_processed_relations:
                entity1 = self.storage.get_entity_by_absolute_id(relation.entity1_absolute_id)
                entity2 = self.storage.get_entity_by_absolute_id(relation.entity2_absolute_id)
                
                if entity1 and entity2:
                    entity1_name = entity1.name
                    entity2_name = entity2.name
                else:
                    entity1_name = relation.entity1_absolute_id
                    entity2_name = relation.entity2_absolute_id
                
                content_preview = relation.content[:80] + '...' if len(relation.content) > 80 else relation.content
                print(f"      - {entity1_name} -- {entity2_name}")
                print(f"        {content_preview}")
        
        if verbose:
            print("  çª—å£å¤„ç†å®Œæˆï¼\n")
    
    def get_statistics(self) -> dict:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç»Ÿè®¡é€»è¾‘
        return {
            "memory_caches": len(list(self.storage.cache_dir.glob("*.json"))),
            "storage_path": str(self.storage.storage_path)
        }
    
    def consolidate_knowledge_graph_entity(self, verbose: bool = True, 
                                    similarity_threshold: float = 0.6,
                                    max_candidates: int = 5,
                                    batch_candidates: Optional[int] = None,
                                    content_snippet_length: int = 64,
                                    parallel: bool = False,
                                    enable_name_match_step: bool = True,
                                    enable_pre_search: Optional[bool] = None) -> dict:
        """
        æ•´ç†çŸ¥è¯†å›¾è°±ï¼šè¯†åˆ«å¹¶åˆå¹¶é‡å¤å®ä½“ï¼Œåˆ›å»ºå…³è”å…³ç³»
        
        å¯¹æ¯ä¸ªå®ä½“ï¼Œåˆ†åˆ«æŒ‰nameå’Œname+contentæœç´¢ç›¸ä¼¼å®ä½“ï¼Œä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦åˆå¹¶æˆ–åˆ›å»ºå…³ç³»è¾¹ã€‚
        
        Args:
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            similarity_threshold: ç›¸ä¼¼åº¦æœç´¢é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼‰
            max_candidates: æ¯æ¬¡æœç´¢è¿”å›çš„æœ€å¤§å€™é€‰å®ä½“æ•°ï¼ˆé»˜è®¤5ï¼‰
            batch_candidates: æ¯æ¬¡æ‰¹é‡å¤„ç†çš„å€™é€‰å®ä½“æ•°ï¼ˆé»˜è®¤Noneï¼Œè¡¨ç¤ºä¸é™åˆ¶ï¼Œä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰max_candidatesä¸ªï¼‰
                            å¦‚æœè®¾ç½®äº†ä¸”å°äºmax_candidatesï¼Œåˆ™åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹å¤„ç†batch_candidatesä¸ª
                            å¦‚æœå¤§äºç­‰äºmax_candidatesï¼Œåˆ™æŒ‰max_candidatesçš„å€¼å¤„ç†
            content_snippet_length: ä¼ å…¥LLMçš„å®ä½“contentæœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤64å­—ç¬¦ï¼‰
            parallel: æ˜¯å¦å¯ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆé»˜è®¤Falseï¼‰
            enable_name_match_step: æ˜¯å¦å¯ç”¨æ­¥éª¤1.5ï¼ˆæŒ‰åç§°å®Œå…¨åŒ¹é…è¿›è¡Œåˆæ­¥æ•´ç†ï¼Œé»˜è®¤Trueï¼‰
            enable_pre_search: æ˜¯å¦å¯ç”¨é¢„æœç´¢ï¼ˆæ­¥éª¤2ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®parallelè‡ªåŠ¨å†³å®šï¼š
                              - parallel=Trueæ—¶ï¼Œå¿…é¡»å¯ç”¨ï¼ˆå¼ºåˆ¶ä¸ºTrueï¼‰
                              - parallel=Falseæ—¶ï¼Œé»˜è®¤å¯ç”¨ï¼ˆTrueï¼‰ï¼Œä½†å¯ä»¥è®¾ç½®ä¸ºFalseæ”¹ä¸ºæŒ‰éœ€æœç´¢
        
        Returns:
            æ•´ç†ç»“æœç»Ÿè®¡ï¼ŒåŒ…å«:
            - entities_analyzed: åˆ†æçš„å®ä½“æ•°é‡
            - entities_merged: åˆå¹¶çš„å®ä½“æ•°é‡
            - alias_relations_created: åˆ›å»ºçš„å…³è”å…³ç³»æ•°é‡
            - merge_details: åˆå¹¶æ“ä½œè¯¦æƒ…åˆ—è¡¨
            - alias_details: å…³è”å…³ç³»è¯¦æƒ…åˆ—è¡¨
        """
        # å¦‚æœå¯ç”¨å¹¶è¡Œå¤„ç†ä¸”çº¿ç¨‹æ•°å¤§äº1ï¼Œä½¿ç”¨å¤šçº¿ç¨‹ç‰ˆæœ¬
        if parallel and self.llm_threads > 1:
            return self._consolidate_knowledge_graph_parallel(
                verbose=verbose,
                similarity_threshold=similarity_threshold,
                max_candidates=max_candidates,
                batch_candidates=batch_candidates,
                content_snippet_length=content_snippet_length
            )
        
        # ç¡®å®šæ˜¯å¦å¯ç”¨é¢„æœç´¢
        # å¦‚æœparallel=Trueï¼Œå¿…é¡»å¯ç”¨é¢„æœç´¢ï¼ˆä½†è¿™ç§æƒ…å†µåº”è¯¥å·²ç»è¿›å…¥ä¸Šé¢çš„å¹¶è¡Œç‰ˆæœ¬ï¼‰
        # å¦‚æœparallel=Falseï¼Œæ ¹æ®enable_pre_searchå‚æ•°å†³å®š
        if enable_pre_search is None:
            # é»˜è®¤å¯ç”¨é¢„æœç´¢ï¼ˆæ‰¹é‡è®¡ç®—æ›´é«˜æ•ˆï¼‰
            use_pre_search = True
        else:
            use_pre_search = enable_pre_search
        
        # ç¡®å®šæ˜¯å¦å¯ç”¨é¢„æœç´¢
        # å¦‚æœparallel=Trueï¼Œå¿…é¡»å¯ç”¨é¢„æœç´¢ï¼ˆä½†è¿™ç§æƒ…å†µåº”è¯¥å·²ç»è¿›å…¥ä¸Šé¢çš„å¹¶è¡Œç‰ˆæœ¬ï¼‰
        # å¦‚æœparallel=Falseï¼Œæ ¹æ®enable_pre_searchå‚æ•°å†³å®š
        if enable_pre_search is None:
            # é»˜è®¤å¯ç”¨é¢„æœç´¢ï¼ˆæ‰¹é‡è®¡ç®—æ›´é«˜æ•ˆï¼‰
            use_pre_search = True
        else:
            use_pre_search = enable_pre_search
        
        if verbose:
            print("=" * 60)
            print("å¼€å§‹çŸ¥è¯†å›¾è°±æ•´ç†...")
            print("=" * 60)
        
        # æ­¥éª¤0ï¼šå¤„ç†è‡ªæŒ‡å‘çš„å…³ç³»ï¼Œå°†å…¶æ€»ç»“åˆ°å®ä½“çš„contentä¸­
        if verbose:
            print(f"\næ­¥éª¤0: å¤„ç†è‡ªæŒ‡å‘çš„å…³ç³»ï¼ˆæ€»ç»“åˆ°å®ä½“contentï¼‰...")
        
        self_ref_relations = self.storage.get_self_referential_relations()
        entities_updated_from_self_ref = 0
        deleted_self_ref_count = 0
        
        if self_ref_relations:
            if verbose:
                print(f"  å‘ç° {len(self_ref_relations)} ä¸ªå®ä½“æœ‰è‡ªæŒ‡å‘å…³ç³»ï¼Œå…± {sum(len(rels) for rels in self_ref_relations.values())} ä¸ªå…³ç³»")
            
            for entity_id, relations in self_ref_relations.items():
                # è·å–å®ä½“çš„æœ€æ–°ç‰ˆæœ¬
                entity = self.storage.get_entity_by_id(entity_id)
                if not entity:
                    continue
                
                # æ”¶é›†æ‰€æœ‰è‡ªæŒ‡å‘å…³ç³»çš„content
                self_ref_contents = [rel['content'] for rel in relations]
                
                if verbose:
                    print(f"    å¤„ç†å®ä½“ {entity.name} ({entity_id})ï¼Œæœ‰ {len(relations)} ä¸ªè‡ªæŒ‡å‘å…³ç³»")
                
                # ç”¨LLMæ€»ç»“è¿™äº›å…³ç³»å†…å®¹åˆ°å®ä½“çš„contentä¸­
                # å°†è‡ªæŒ‡å‘å…³ç³»çš„å†…å®¹è§†ä¸ºå®ä½“çš„å±æ€§ä¿¡æ¯
                summarized_content = self.llm_client.merge_entity_content(
                    old_content=entity.content,
                    new_content="\n\n".join([f"å±æ€§ä¿¡æ¯ï¼š{content}" for content in self_ref_contents])
                )
                
                # æ›´æ–°å®ä½“çš„æœ€æ–°ç‰ˆæœ¬ï¼ˆåˆ›å»ºæ–°ç‰ˆæœ¬ï¼‰
                from datetime import datetime
                new_entity_id = f"entity_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
                new_entity = Entity(
                    id=new_entity_id,
                    entity_id=entity.entity_id,
                    name=entity.name,
                    content=summarized_content,
                    physical_time=datetime.now(),
                    memory_cache_id=entity.memory_cache_id,
                    doc_name=entity.doc_name if hasattr(entity, 'doc_name') else ""
                )
                self.storage.save_entity(new_entity)
                
                entities_updated_from_self_ref += 1
                deleted_self_ref_count += len(relations)
                
                if verbose:
                    print(f"      å·²å°† {len(relations)} ä¸ªè‡ªæŒ‡å‘å…³ç³»æ€»ç»“åˆ°å®ä½“contentä¸­")
            
            # åˆ é™¤æ‰€æœ‰è‡ªæŒ‡å‘çš„å…³ç³»
            actual_deleted = self.storage.delete_self_referential_relations()
            if verbose:
                print(f"  å·²åˆ é™¤ {actual_deleted} ä¸ªè‡ªæŒ‡å‘çš„å…³ç³»")
        else:
            if verbose:
                print(f"  æœªå‘ç°è‡ªæŒ‡å‘çš„å…³ç³»")
        
        # ç»“æœç»Ÿè®¡
        result = {
            "entities_analyzed": 0,
            "entities_merged": 0,
            "alias_relations_created": 0,
            "alias_relations_updated": 0,  # æ–°å¢ï¼šæ›´æ–°çš„å…³ç³»æ•°é‡
            "self_referential_relations_processed": deleted_self_ref_count,  # å¤„ç†çš„è‡ªæŒ‡å‘å…³ç³»æ•°é‡
            "entities_updated_from_self_ref": entities_updated_from_self_ref,  # å› è‡ªæŒ‡å‘å…³ç³»è€Œæ›´æ–°çš„å®ä½“æ•°é‡
            "merge_details": [],
            "alias_details": []
        }
        
        # æ­¥éª¤1ï¼šè·å–æ‰€æœ‰å®ä½“
        if verbose:
            print(f"\næ­¥éª¤1: è·å–æ‰€æœ‰å®ä½“...")
        
        all_entities = self.storage.get_all_entities()
        
        if not all_entities:
            if verbose:
                print("  çŸ¥è¯†åº“ä¸­æ²¡æœ‰å®ä½“ã€‚")
            return result
        
        # æŒ‰ç‰ˆæœ¬æ•°é‡ä»å¤§åˆ°å°æ’åº
        entity_ids = [entity.entity_id for entity in all_entities]
        version_counts = self.storage.get_entity_version_counts(entity_ids)
        all_entities.sort(key=lambda e: version_counts.get(e.entity_id, 0), reverse=True)
        
        # è®°å½•æ•´ç†å‰çš„å®ä½“æ€»æ•°
        initial_entity_count = len(all_entities)
        if verbose:
            print(f"  æ•´ç†å‰å…±æœ‰ {initial_entity_count} ä¸ªå®ä½“")
        
        # è®°å½•å·²åˆå¹¶çš„å®ä½“IDï¼ˆç”¨äºåç»­embeddingæœç´¢æ—¶æ’é™¤ï¼‰
        merged_entity_ids = set()
        # è®°å½•åˆå¹¶æ˜ å°„ï¼šsource_entity_id -> target_entity_id
        merge_mapping = {}
        
        # æ­¥éª¤1.5ï¼šå…ˆæŒ‰åç§°å®Œå…¨åŒ¹é…è¿›è¡Œæ•´ç†
        if enable_name_match_step:
            if verbose:
                print(f"\næ­¥éª¤1.5: æŒ‰åç§°å®Œå…¨åŒ¹é…è¿›è¡Œåˆæ­¥æ•´ç†...")
            
            # æ„å»ºåç§°åˆ°å®ä½“åˆ—è¡¨çš„æ˜ å°„
            name_to_entities = {}
            for entity in all_entities:
                name = entity.name
                if name not in name_to_entities:
                    name_to_entities[name] = []
                name_to_entities[name].append(entity)
            
            # å¯¹æ¯ä¸ªåç§°ç»„å†…çš„å®ä½“æŒ‰ç‰ˆæœ¬æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
            for name in name_to_entities:
                name_to_entities[name].sort(
                    key=lambda e: version_counts.get(e.entity_id, 0), 
                    reverse=True
                )
            
            # æŒ‰ç…§æ¯ä¸ªåç§°ç»„ä¸­å®ä½“çš„æœ€å¤§ç‰ˆæœ¬æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰ï¼Œç„¶åæŒ‰é¡ºåºå¤„ç†
            name_groups_sorted = sorted(
                name_to_entities.items(),
                key=lambda item: max(
                    (version_counts.get(e.entity_id, 0) for e in item[1]),
                    default=0
                ),
                reverse=True
            )
            
            # å¤„ç†åç§°å®Œå…¨ä¸€è‡´çš„å®ä½“ç»„
            name_match_count = 0
            for name, entities_with_same_name in name_groups_sorted:
                # åªå¤„ç†æœ‰å¤šä¸ªå®ä½“çš„åç§°ç»„
                if len(entities_with_same_name) <= 1:
                    continue
                
                name_match_count += 1
                if verbose:
                    print(f"  å‘ç°åç§°å®Œå…¨ä¸€è‡´çš„å®ä½“ç»„: {name} (å…± {len(entities_with_same_name)} ä¸ªå®ä½“)")
                
                # å‡†å¤‡å®ä½“ä¿¡æ¯ç”¨äºLLMåˆ¤æ–­
                entities_info = []
                for entity in entities_with_same_name:
                    # è·³è¿‡å·²åˆå¹¶çš„å®ä½“
                    if entity.entity_id in merged_entity_ids:
                        continue
                    
                    version_count = version_counts.get(entity.entity_id, 0)
                    entities_info.append({
                        "entity_id": entity.entity_id,
                        "name": entity.name,
                        "content": entity.content,
                        "version_count": version_count
                    })
                
                # å¦‚æœè¿‡æ»¤ååªå‰©ä¸€ä¸ªæˆ–æ²¡æœ‰å®ä½“ï¼Œè·³è¿‡
                if len(entities_info) <= 1:
                    continue
                
                # è·å–è®°å¿†ä¸Šä¸‹æ–‡
                memory_contexts = {}
                for entity in entities_with_same_name:
                    if entity.entity_id in merged_entity_ids:
                        continue
                    cache_text = self.storage.get_memory_cache_text(entity.memory_cache_id)
                    if cache_text:
                        memory_contexts[entity.entity_id] = cache_text
                
                # æ£€æŸ¥å®ä½“å¯¹ä¹‹é—´æ˜¯å¦å·²æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“åˆ™ç›´æ¥åˆå¹¶
                entity_ids_for_check = [info['entity_id'] for info in entities_info]
                existing_relations_between = self._check_and_merge_entities_from_relations(
                    entity_ids_for_check,
                    entities_info,
                    version_counts,
                    merged_entity_ids,
                    merge_mapping,
                    result,
                    verbose
                )
                
                if verbose and existing_relations_between:
                    print(f"    å‘ç° {len(existing_relations_between)} å¯¹å®ä½“ä¹‹é—´å·²æœ‰å…³ç³»ï¼Œå°†äº¤ç”±LLMåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶")
                
                # è°ƒç”¨LLMåˆ†æï¼šåˆ¤æ–­æ˜¯åˆå¹¶è¿˜æ˜¯å…³è”å…³ç³»
                analysis_result = self.llm_client.analyze_entity_duplicates(
                    entities_info,
                    memory_contexts,
                    content_snippet_length=content_snippet_length,
                    existing_relations_between_entities=existing_relations_between
                )
                
                if "error" in analysis_result:
                    if verbose:
                        print(f"    åˆ†æå¤±è´¥ï¼Œè·³è¿‡è¯¥ç»„")
                    continue
            
            # å¤„ç†åˆå¹¶ï¼ˆè¿‡æ»¤æ‰å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼‰
            merge_groups = analysis_result.get("merge_groups", [])
            for merge_group in merge_groups:
                target_entity_id = merge_group.get("target_entity_id")
                source_entity_ids = merge_group.get("source_entity_ids", [])
                reason = merge_group.get("reason", "")
                
                if not target_entity_id or not source_entity_ids:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                if any(sid in merged_entity_ids for sid in source_entity_ids):
                    continue
                
                # ä¸å†è¿‡æ»¤å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼Œè®©LLMåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                # å³ä½¿æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œä¹Ÿåº”è¯¥åˆå¹¶
                
                # æ‰§è¡Œåˆå¹¶
                merge_result = self.storage.merge_entity_ids(target_entity_id, source_entity_ids)
                merge_result["reason"] = reason
                
                if verbose:
                    target_name = next((e.name for e in entities_with_same_name if e.entity_id == target_entity_id), target_entity_id)
                    print(f"    åˆå¹¶å®ä½“: {target_name} ({target_entity_id}) <- {len(source_entity_ids)} ä¸ªæºå®ä½“")
                    print(f"      åŸå› : {reason}")
                
                # å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
                self._handle_self_referential_relations_after_merge(target_entity_id, verbose)
                
                # è®°å½•å·²åˆå¹¶çš„å®ä½“å’Œåˆå¹¶æ˜ å°„
                for sid in source_entity_ids:
                    merged_entity_ids.add(sid)
                    merge_mapping[sid] = target_entity_id
                
                # æ›´æ–°ç»“æœç»Ÿè®¡
                result["merge_details"].append(merge_result)
                result["entities_merged"] += merge_result.get("entities_updated", 0)
            
            # å¤„ç†å…³ç³»ï¼ˆåˆ«åå…³ç³»ï¼‰
            alias_relations = analysis_result.get("alias_relations", [])
            for alias_info in alias_relations:
                entity1_id = alias_info.get("entity1_id")
                entity2_id = alias_info.get("entity2_id")
                entity1_name = alias_info.get("entity1_name", "")
                entity2_name = alias_info.get("entity2_name", "")
                preliminary_content = alias_info.get("content")
                
                if not entity1_id or not entity2_id:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶ï¼ˆå¦‚æœå·²åˆå¹¶ï¼Œéœ€è¦æ‰¾åˆ°åˆå¹¶åçš„å®é™…IDï¼‰
                actual_entity1_id = merge_mapping.get(entity1_id, entity1_id)
                actual_entity2_id = merge_mapping.get(entity2_id, entity2_id)
                
                # å¦‚æœå®ä½“å·²è¢«åˆå¹¶ï¼Œè·³è¿‡ï¼ˆå› ä¸ºåˆå¹¶åçš„å®ä½“å¯èƒ½ä¸åœ¨å½“å‰åç§°ç»„ä¸­ï¼‰
                if entity1_id in merged_entity_ids or entity2_id in merged_entity_ids:
                    if verbose:
                        print(f"    è·³è¿‡å…³ç³»ï¼ˆå®ä½“å·²åˆå¹¶ï¼‰: {entity1_name} -> {entity2_name}")
                    continue
                
                # å¤„ç†å…³ç³»
                rel_info = {
                    "entity1_id": entity1_id,
                    "entity2_id": entity2_id,
                    "actual_entity1_id": actual_entity1_id,
                    "actual_entity2_id": actual_entity2_id,
                    "entity1_name": entity1_name,
                    "entity2_name": entity2_name,
                    "content": preliminary_content
                }
                
                rel_result = self._process_single_alias_relation(rel_info, verbose=False)
                if rel_result:
                    result["alias_details"].append(rel_result)
                    if rel_result.get("is_new"):
                        result["alias_relations_created"] += 1
                    elif rel_result.get("is_updated"):
                        result["alias_relations_updated"] += 1
            
            if verbose:
                print(f"  åç§°åŒ¹é…å®Œæˆï¼Œå¤„ç†äº† {name_match_count} ä¸ªåç§°ç»„ï¼Œåˆå¹¶äº† {len(merged_entity_ids)} ä¸ªå®ä½“")
        else:
            if verbose:
                print(f"\næ­¥éª¤1.5: è·³è¿‡ï¼ˆå·²ç¦ç”¨ï¼‰")
        
        # æ­¥éª¤1.5ä¹‹åï¼Œé‡æ–°æŒ‰ç‰ˆæœ¬æ•°é‡ä»å¤§åˆ°å°æ’åºï¼ˆå› ä¸ºåˆå¹¶å¯èƒ½æ”¹å˜äº†ç‰ˆæœ¬æ•°ï¼‰
        entity_ids = [entity.entity_id for entity in all_entities]
        version_counts = self.storage.get_entity_version_counts(entity_ids)
        all_entities.sort(key=lambda e: version_counts.get(e.entity_id, 0), reverse=True)
        
        # ç”¨äºç´¯ç§¯æ‰€æœ‰åˆ†æè¿‡çš„å®ä½“ä¿¡æ¯ï¼ˆç”¨äºæœ€ç»ˆä¿å­˜åˆ°JSONçš„textå­—æ®µï¼‰
        all_analyzed_entities_text = []
        
        # è®°å½•å·²å¤„ç†çš„entity_idå¯¹ï¼Œé¿å…é‡å¤åˆ†æ
        processed_pairs = set()
        
        # æ­¥éª¤2ï¼šä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼ä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“ï¼ˆå¯é€‰ï¼‰
        entity_to_candidates = {}
        
        if use_pre_search:
            if verbose:
                print(f"\næ­¥éª¤2: ä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼é¢„æœç´¢æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“ï¼ˆé˜ˆå€¼: {similarity_threshold}, æœ€å¤§å€™é€‰æ•°: {max_candidates}ï¼‰...")
                print(f"  ä½¿ç”¨å¤šç§æ£€ç´¢æ¨¡å¼ï¼šname_only(embedding) + name_and_content(embedding) + name_only(text/jaccard)")
            
            # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(current: int, total: int, entity_name: str):
                if verbose and current % max(1, total // 20) == 0 or current == total:  # æ¯5%æˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºä¸€æ¬¡
                    percentage = (current / total) * 100
                    print(f"  é¢„æœç´¢è¿›åº¦: [{current}/{total}] ({percentage:.1f}%) - å½“å‰å¤„ç†: {entity_name[:30]}...")
            
            # ä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼ä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“
            entity_to_candidates = self.storage.find_related_entities_by_embedding(
                similarity_threshold=similarity_threshold,
                max_candidates=max_candidates,
                use_mixed_search=True,  # å¯ç”¨æ··åˆæ£€ç´¢
                content_snippet_length=content_snippet_length,
                progress_callback=progress_callback if verbose else None
            )
            
            # è¿‡æ»¤æ‰å·²åˆå¹¶çš„å®ä½“ï¼ˆåœ¨å€™é€‰åˆ—è¡¨ä¸­æ’é™¤ï¼‰
            for entity_id in list(entity_to_candidates.keys()):
                # å¦‚æœå½“å‰å®ä½“å·²åˆå¹¶ï¼Œä»å€™é€‰åˆ—è¡¨ä¸­ç§»é™¤
                if entity_id in merged_entity_ids:
                    del entity_to_candidates[entity_id]
                    continue
                
                # ä»å€™é€‰åˆ—è¡¨ä¸­æ’é™¤å·²åˆå¹¶çš„å®ä½“
                candidates = entity_to_candidates[entity_id]
                entity_to_candidates[entity_id] = candidates - merged_entity_ids
            
            if verbose:
                total_candidates = sum(len(candidates) for candidates in entity_to_candidates.values())
                print(f"  é¢„æœç´¢å®Œæˆï¼Œå…± {len(entity_to_candidates)} ä¸ªå®ä½“ï¼Œæ‰¾åˆ° {total_candidates} ä¸ªå…³è”å®ä½“ï¼ˆå·²æ’é™¤ {len(merged_entity_ids)} ä¸ªå·²åˆå¹¶å®ä½“ï¼‰")
        else:
            if verbose:
                print(f"\næ­¥éª¤2: è·³è¿‡é¢„æœç´¢ï¼Œå°†æŒ‰éœ€æœç´¢æ¯ä¸ªå®ä½“çš„å…³è”å®ä½“")
        
        if verbose:
            print(f"\næ­¥éª¤3: é€ä¸ªå®ä½“åˆ†æå¹¶å¤„ç†...")
        
        for entity_idx, entity in enumerate(all_entities, 1):
            # è·³è¿‡å·²è¢«åˆå¹¶çš„å®ä½“
            if entity.entity_id in merged_entity_ids:
                continue
            
            if verbose:
                # è·å–å®ä½“çš„ç‰ˆæœ¬æ•°
                entity_version_count = version_counts.get(entity.entity_id, 0)
                print(f"\n  [{entity_idx}/{len(all_entities)}] åˆ†æå®ä½“: {entity.name} (entity_id: {entity.entity_id}, ç‰ˆæœ¬æ•°: {entity_version_count})")
            
            # è·å–å€™é€‰å®ä½“ï¼šå¦‚æœå¯ç”¨äº†é¢„æœç´¢ï¼Œä»é¢„æœç´¢ç»“æœä¸­è·å–ï¼›å¦åˆ™æŒ‰éœ€æœç´¢
            if use_pre_search:
                candidate_entity_ids = entity_to_candidates.get(entity.entity_id, set())
            else:
                # æŒ‰éœ€æœç´¢ï¼šä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼æœç´¢å½“å‰å®ä½“çš„å…³è”å®ä½“
                candidate_entity_ids = set()

                # æ¨¡å¼1ï¼šåªç”¨nameæ£€ç´¢ï¼ˆä½¿ç”¨embeddingï¼‰
                candidates_name_jaccard = self.storage.search_entities_by_similarity(
                    query_name=entity.name,
                    query_content=None,
                    threshold=0.0,
                    max_results=max_candidates,
                    content_snippet_length=content_snippet_length,
                    text_mode="name_only",
                    similarity_method="jaccard"
                )
                
                # æ¨¡å¼1ï¼šåªç”¨nameæ£€ç´¢ï¼ˆä½¿ç”¨embeddingï¼‰
                candidates_name_embedding = self.storage.search_entities_by_similarity(
                    query_name=entity.name,
                    query_content=None,
                    threshold=similarity_threshold,
                    max_results=max_candidates,
                    content_snippet_length=content_snippet_length,
                    text_mode="name_only",
                    similarity_method="embedding"
                )
                
                # æ¨¡å¼2ï¼šä½¿ç”¨name+contentæ£€ç´¢ï¼ˆä½¿ç”¨embeddingï¼‰
                candidates_full_embedding = self.storage.search_entities_by_similarity(
                    query_name=entity.name,
                    query_content=entity.content,
                    threshold=similarity_threshold,
                    max_results=max_candidates,
                    content_snippet_length=content_snippet_length,
                    text_mode="name_and_content",
                    similarity_method="embedding"
                )
                
                # åˆå¹¶å€™é€‰å®ä½“å¹¶å»é‡ï¼ˆæŒ‰entity_idå»é‡ï¼Œä¿ç•™æ¯ä¸ªentity_idçš„æœ€æ–°ç‰ˆæœ¬ï¼‰
                candidate_dict = {}
                for candidate in candidates_name_jaccard + candidates_name_embedding + candidates_full_embedding:
                    if candidate.entity_id == entity.entity_id:
                        continue  # è·³è¿‡è‡ªå·±
                    if candidate.entity_id not in candidate_dict:
                        candidate_dict[candidate.entity_id] = candidate
                    else:
                        # ä¿ç•™ç‰©ç†æ—¶é—´æœ€æ–°çš„
                        if candidate.physical_time > candidate_dict[candidate.entity_id].physical_time:
                            candidate_dict[candidate.entity_id] = candidate
                
                # æå–entity_idåˆ°setä¸­
                candidate_entity_ids = {cid for cid in candidate_dict.keys()}
            
            # è¿‡æ»¤æ‰å·²å¤„ç†çš„é…å¯¹å’Œå·²åˆå¹¶çš„å®ä½“
            candidate_entity_ids = {
                cid for cid in candidate_entity_ids 
                if cid not in merged_entity_ids and 
                   (min(entity.entity_id, cid), max(entity.entity_id, cid)) not in processed_pairs
            }
            
            if not candidate_entity_ids:
                if verbose:
                    print(f"    æœªæ‰¾åˆ°ç›¸ä¼¼å®ä½“å€™é€‰")
                continue
            
            # ç¡®å®šæ‰¹é‡å¤„ç†çš„å¤§å°
            if batch_candidates is not None and batch_candidates < max_candidates:
                batch_size = batch_candidates
            else:
                batch_size = max_candidates
            
            # å°†å€™é€‰å®ä½“è½¬æ¢ä¸ºåˆ—è¡¨å¹¶åˆ†æ‰¹å¤„ç†
            candidate_entity_ids_list = list(candidate_entity_ids)
            total_candidates = len(candidate_entity_ids_list)
            total_batches = (total_candidates + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
            
            if verbose:
                print(f"    æ‰¾åˆ° {total_candidates} ä¸ªå€™é€‰å®ä½“ï¼Œå°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} ä¸ªï¼‰")
            
            # å‡†å¤‡å½“å‰å®ä½“ä¿¡æ¯ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å…±äº«ï¼‰
            current_version_count = self.storage.get_entity_version_count(entity.entity_id)
            current_entity_info = {
                "entity_id": entity.entity_id,
                "name": entity.name,
                "content": entity.content,
                "version_count": current_version_count
            }
            
            # ========== é˜¶æ®µ1: åˆ†æ‰¹åˆæ­¥ç­›é€‰ï¼ˆåªæ”¶é›†å€™é€‰ï¼Œä¸æ‰§è¡Œæ“ä½œï¼‰ ==========
            # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„å€™é€‰
            all_possible_merges = []  # æ‰€æœ‰å¯èƒ½éœ€è¦åˆå¹¶çš„å€™é€‰
            all_possible_relations = []  # æ‰€æœ‰å¯èƒ½éœ€è¦åˆ›å»ºå…³ç³»çš„å€™é€‰
            all_candidates_full_info = {}  # æ‰€æœ‰å€™é€‰å®ä½“çš„å®Œæ•´ä¿¡æ¯
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, total_candidates)
                batch_candidate_ids = candidate_entity_ids_list[start_idx:end_idx]
                
                if verbose:
                    print(f"\n    [åˆæ­¥ç­›é€‰] ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼ˆ{len(batch_candidate_ids)} ä¸ªå€™é€‰å®ä½“ï¼‰...")
                
                # è·å–å½“å‰æ‰¹æ¬¡çš„å€™é€‰å®ä½“å®Œæ•´ä¿¡æ¯
                candidates_info = []
                for cid in batch_candidate_ids:
                    candidate_entity = self.storage.get_entity_by_id(cid)
                    if candidate_entity:
                        version_count = self.storage.get_entity_version_count(cid)
                        info = {
                            "entity_id": cid,
                            "name": candidate_entity.name,
                            "content": candidate_entity.content,
                            "version_count": version_count
                        }
                        candidates_info.append(info)
                        all_candidates_full_info[cid] = info
                        # è®°å½•å·²å¤„ç†çš„é…å¯¹
                        pair = (min(entity.entity_id, cid), max(entity.entity_id, cid))
                        processed_pairs.add(pair)
                
                if not candidates_info:
                    continue
                
                # æŒ‰ç‰ˆæœ¬æ•°é‡ä»å¤§åˆ°å°æ’åºå€™é€‰å®ä½“
                candidates_info.sort(key=lambda x: x.get('version_count', 0), reverse=True)
                
                # æ„å»ºåˆ†æç»„ï¼šå½“å‰å®ä½“ + å½“å‰æ‰¹æ¬¡çš„å€™é€‰å®ä½“
                entities_for_analysis = [current_entity_info] + candidates_info
                
                if verbose:
                    print(f"      å½“å‰æ‰¹æ¬¡å€™é€‰å®ä½“:")
                    for info in candidates_info:
                        print(f"        - {info['name']} (entity_id: {info['entity_id']}, versions: {info['version_count']})")
                
                # åˆæ­¥ç­›é€‰ï¼ˆä½¿ç”¨snippetï¼‰- åªæ”¶é›†å€™é€‰ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
                preliminary_result = self.llm_client.analyze_entity_candidates_preliminary(
                    entities_for_analysis,
                    content_snippet_length=content_snippet_length
                )
                
                possible_merges = preliminary_result.get("possible_merges", [])
                possible_relations = preliminary_result.get("possible_relations", [])
                no_action = preliminary_result.get("no_action", [])
                preliminary_summary = preliminary_result.get("analysis_summary", "")
                
                if verbose:
                    if preliminary_summary:
                        print(f"      åˆæ­¥ç­›é€‰ç»“æœ: {preliminary_summary[:100]}..." if len(preliminary_summary) > 100 else f"      åˆæ­¥ç­›é€‰ç»“æœ: {preliminary_summary}")
                    print(f"      å¯èƒ½éœ€è¦åˆå¹¶: {len(possible_merges)} ä¸ª, å¯èƒ½å­˜åœ¨å…³ç³»: {len(possible_relations)} ä¸ª, ä¸å¤„ç†: {len(no_action)} ä¸ª")
                
                # æ”¶é›†å€™é€‰ï¼ˆè®°å½•å½“å‰å®ä½“å’Œå€™é€‰å®ä½“çš„é…å¯¹ï¼‰
                for item in possible_merges:
                    cid = item.get("entity_id") if isinstance(item, dict) else item
                    if cid and cid not in merged_entity_ids:
                        all_possible_merges.append({
                            "current_entity_id": entity.entity_id,
                            "current_entity_info": current_entity_info,
                            "candidate_entity_id": cid,
                            "reason": item.get("reason", "") if isinstance(item, dict) else ""
                        })
                
                for item in possible_relations:
                    cid = item.get("entity_id") if isinstance(item, dict) else item
                    if cid and cid not in merged_entity_ids:
                        all_possible_relations.append({
                            "current_entity_id": entity.entity_id,
                            "current_entity_info": current_entity_info,
                            "candidate_entity_id": cid,
                            "reason": item.get("reason", "") if isinstance(item, dict) else ""
                        })
            
            # ========== é˜¶æ®µ2: ç²¾ç»†åŒ–åˆ¤æ–­ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼‰ ==========
            # å¯¹äºè¢«åˆ¤æ–­ä¸ºå…³ç³»çš„å€™é€‰ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰å…³ç³»ï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡ç²¾ç»†åŒ–åˆ¤æ–­
            filtered_possible_relations = []
            skipped_relations_count = 0
            for item in all_possible_relations:
                cid = item["candidate_entity_id"]
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…³ç³»
                existing_rels = self.storage.get_relations_by_entities(
                    entity.entity_id,
                    cid
                )
                if existing_rels and len(existing_rels) > 0:
                    # å·²æœ‰å…³ç³»ï¼Œè·³è¿‡ç²¾ç»†åŒ–åˆ¤æ–­
                    skipped_relations_count += 1
                    if verbose:
                        # è·å–å€™é€‰å®ä½“åç§°
                        candidate_name = cid
                        if cid in all_candidates_full_info:
                            candidate_name = all_candidates_full_info[cid].get('name', cid)
                        else:
                            candidate_entity = self.storage.get_entity_by_id(cid)
                            if candidate_entity:
                                candidate_name = candidate_entity.name
                        print(f"      è·³è¿‡å·²æœ‰å…³ç³»: {entity.name} <-> {candidate_name} (å·²æœ‰ {len(existing_rels)} ä¸ªå…³ç³»)")
                else:
                    # æ²¡æœ‰å…³ç³»ï¼Œéœ€è¦ç²¾ç»†åŒ–åˆ¤æ–­
                    filtered_possible_relations.append(item)
            
            if verbose:
                total_candidates_to_analyze = len(all_possible_merges) + len(filtered_possible_relations)
                print(f"\n    [ç²¾ç»†åŒ–åˆ¤æ–­] å…± {total_candidates_to_analyze} ä¸ªå€™é€‰éœ€è¦ç²¾ç»†åŒ–åˆ¤æ–­...")
                print(f"      å¯èƒ½åˆå¹¶: {len(all_possible_merges)} ä¸ª")
                print(f"      å¯èƒ½å…³ç³»: {len(filtered_possible_relations)} ä¸ª (è·³è¿‡å·²æœ‰å…³ç³»: {skipped_relations_count} ä¸ª)")
            
            # åˆå¹¶å¯èƒ½åˆå¹¶å’Œå¯èƒ½å…³ç³»çš„å€™é€‰ï¼ˆå»é‡ï¼‰
            all_candidates_to_analyze = {}
            for item in all_possible_merges + filtered_possible_relations:
                cid = item["candidate_entity_id"]
                if cid not in all_candidates_to_analyze:
                    all_candidates_to_analyze[cid] = item
            
            # å¯¹æ¯ä¸ªå€™é€‰è¿›è¡Œç²¾ç»†åŒ–åˆ¤æ–­
            merge_decisions = []  # ç²¾ç»†åŒ–åˆ¤æ–­åç¡®å®šè¦åˆå¹¶çš„
            relation_decisions = []  # ç²¾ç»†åŒ–åˆ¤æ–­åç¡®å®šè¦åˆ›å»ºå…³ç³»çš„
            
            for cid, item in all_candidates_to_analyze.items():
                if cid not in all_candidates_full_info:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                if cid in merged_entity_ids:
                    continue
                
                candidate_info = all_candidates_full_info[cid]
                
                # è·å–ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å·²æœ‰å…³ç³»
                existing_rels = self.storage.get_relations_by_entities(
                    entity.entity_id,
                    cid
                )
                existing_relations_list = []
                if existing_rels:
                    # å»é‡ï¼Œæ¯ä¸ªrelation_idåªä¿ç•™æœ€æ–°ç‰ˆæœ¬
                    rel_dict = {}
                    for rel in existing_rels:
                        if rel.relation_id not in rel_dict or rel.physical_time > rel_dict[rel.relation_id].physical_time:
                            rel_dict[rel.relation_id] = rel
                    for rel in rel_dict.values():
                        existing_relations_list.append({
                            "relation_id": rel.relation_id,
                            "content": rel.content
                        })
                
                # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨å½“å‰å®ä½“çš„memory_cacheï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å€™é€‰å®ä½“çš„ï¼‰
                context_text = None
                if entity.memory_cache_id:
                    context_text = self.storage.get_memory_cache_text(entity.memory_cache_id)
                if not context_text:
                    candidate_entity = self.storage.get_entity_by_id(cid)
                    if candidate_entity and candidate_entity.memory_cache_id:
                        context_text = self.storage.get_memory_cache_text(candidate_entity.memory_cache_id)
                
                if verbose:
                    print(f"      ç²¾ç»†åŒ–åˆ¤æ–­: {entity.name} vs {candidate_info['name']}")
                    if existing_relations_list:
                        print(f"        å·²æœ‰ {len(existing_relations_list)} ä¸ªå…³ç³»")
                
                # è°ƒç”¨ç²¾ç»†åŒ–åˆ¤æ–­ï¼ˆä¼ å…¥ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼‰
                detailed_result = self.llm_client.analyze_entity_pair_detailed(
                    current_entity_info,
                    candidate_info,
                    existing_relations_list,
                    context_text=context_text
                )
                
                action = detailed_result.get("action", "no_action")
                reason = detailed_result.get("reason", "")
                
                if verbose:
                    print(f"        åˆ¤æ–­ç»“æœ: {action}")
                    print(f"        ç†ç”±: {reason[:80]}..." if len(reason) > 80 else f"        ç†ç”±: {reason}")
                
                if action == "merge":
                    merge_target = detailed_result.get("merge_target", "")
                    # ç¡®å®šåˆå¹¶æ–¹å‘ï¼ˆç‰ˆæœ¬å¤šçš„ä½œä¸ºtargetï¼‰
                    if not merge_target:
                        if current_entity_info["version_count"] >= candidate_info["version_count"]:
                            merge_target = entity.entity_id
                        else:
                            merge_target = cid
                    
                    merge_decisions.append({
                        "target_entity_id": merge_target,
                        "source_entity_id": cid if merge_target == entity.entity_id else entity.entity_id,
                        "source_name": candidate_info["name"],
                        "target_name": entity.name if merge_target == entity.entity_id else candidate_info["name"],
                        "reason": reason
                    })
                elif action == "create_relation":
                    relation_content = detailed_result.get("relation_content", "")
                    relation_decisions.append({
                        "entity1_id": entity.entity_id,
                        "entity2_id": cid,
                        "entity1_name": entity.name,
                        "entity2_name": candidate_info["name"],
                        "content": relation_content,
                        "reason": reason
                    })
            
            result["entities_analyzed"] += 1
            
            # æ„å»ºåŒ…å«å®Œæ•´entityä¿¡æ¯çš„text
            all_entities_info = [current_entity_info] + list(all_candidates_full_info.values())
            entity_list_text = self._build_entity_list_text(all_entities_info)
            all_analyzed_entities_text.append(f"\n\n{'='*80}\nåˆ†æå®ä½“: {entity.name} ({entity.entity_id})\n{'='*80}\n")
            all_analyzed_entities_text.append(entity_list_text)
            
            if verbose:
                print(f"\n    [ç²¾ç»†åŒ–åˆ¤æ–­å®Œæˆ]")
                print(f"      ç¡®å®šéœ€è¦åˆå¹¶: {len(merge_decisions)} ä¸ª")
                print(f"      ç¡®å®šéœ€è¦åˆ›å»ºå…³ç³»: {len(relation_decisions)} ä¸ª")
            
            # ========== é˜¶æ®µ3: æ‰§è¡Œæ“ä½œï¼ˆç²¾ç»†åŒ–åˆ¤æ–­å…¨éƒ¨å®Œæˆåï¼‰ ==========
            if verbose and (merge_decisions or relation_decisions):
                print(f"\n    [æ‰§è¡Œæ“ä½œ]...")
            
            final_target_id = None  # ç”¨äºåç»­åˆ›å»ºå…³è”å…³ç³»æ—¶ä½¿ç”¨
            all_merged_in_this_round = set()  # æœ¬æ¬¡å¾ªç¯ä¸­è¢«åˆå¹¶çš„å®ä½“ID
            
            # è½¬æ¢ä¸ºæ—§æ ¼å¼çš„merge_groupsä»¥å¤ç”¨åç»­ä»£ç 
            merge_groups = []
            for md in merge_decisions:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒtargetçš„ç»„
                found = False
                for mg in merge_groups:
                    if mg["target_entity_id"] == md["target_entity_id"]:
                        if md["source_entity_id"] not in mg["source_entity_ids"]:
                            mg["source_entity_ids"].append(md["source_entity_id"])
                            mg["reason"] += f"; {md['reason']}"
                        found = True
                        break
                if not found:
                    merge_groups.append({
                        "target_entity_id": md["target_entity_id"],
                        "source_entity_ids": [md["source_entity_id"]],
                        "reason": md["reason"]
                    })
            
            # è½¬æ¢ä¸ºæ—§æ ¼å¼çš„alias_relations
            alias_relations = relation_decisions
            
            # æ„å»ºentities_for_analysisï¼ˆç”¨äºåç»­å…³ç³»å¤„ç†ï¼‰
            entities_for_analysis = [current_entity_info] + list(all_candidates_full_info.values())
            
            if merge_groups:
                if verbose:
                    print(f"      æ‰§è¡Œåˆå¹¶æ“ä½œ...")
                
                # æ”¶é›†æ‰€æœ‰éœ€è¦åˆå¹¶çš„å®ä½“IDï¼ˆåŒ…æ‹¬targetå’Œsourceï¼‰
                all_merge_entity_ids = set()
                merge_reasons = []
                
                for merge_info in merge_groups:
                    target_id = merge_info.get("target_entity_id")
                    source_ids = merge_info.get("source_entity_ids", [])
                    reason = merge_info.get("reason", "")
                    
                    if not target_id or not source_ids:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                    if any(sid in merged_entity_ids for sid in source_ids):
                        if verbose:
                            print(f"        è·³è¿‡å·²åˆå¹¶çš„å®ä½“: {source_ids}")
                        continue
                    
                    # ä¸å†è¿‡æ»¤å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼Œè®©LLMåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                    # å³ä½¿æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œä¹Ÿåº”è¯¥åˆå¹¶
                    
                    # æ”¶é›†æ‰€æœ‰éœ€è¦åˆå¹¶çš„å®ä½“
                    all_merge_entity_ids.add(target_id)
                    all_merge_entity_ids.update(source_ids)
                    if reason:
                        merge_reasons.append(reason)
                
                if all_merge_entity_ids:
                    # ç¡®å®šæœ€ç»ˆçš„targetï¼šé€‰æ‹©ç‰ˆæœ¬æ•°æœ€å¤šçš„å®ä½“
                    target_candidates = []
                    for eid in all_merge_entity_ids:
                        version_count = self.storage.get_entity_version_count(eid)
                        target_candidates.append((eid, version_count))
                    
                    # æŒ‰ç‰ˆæœ¬æ•°æ’åºï¼Œé€‰æ‹©æœ€å¤šçš„ä½œä¸ºtarget
                    target_candidates.sort(key=lambda x: x[1], reverse=True)
                    final_target_id = target_candidates[0][0]
                    final_target_versions = target_candidates[0][1]
                    
                    # å…¶ä»–å®ä½“éƒ½æ˜¯source
                    final_source_ids = [eid for eid, _ in target_candidates[1:]]
                    
                    if final_source_ids:
                        # è·å–å®ä½“åç§°ç”¨äºæ˜¾ç¤º
                        target_entity = self.storage.get_entity_by_id(final_target_id)
                        target_name = target_entity.name if target_entity else final_target_id
                        
                        # åˆå¹¶æ‰€æœ‰åŸå› 
                        combined_reason = "ï¼›".join(merge_reasons) if merge_reasons else "å¤šä¸ªå®ä½“éœ€è¦åˆå¹¶"
                        
                        if verbose:
                            print(f"      åˆå¹¶å¤šä¸ªå®ä½“åˆ°ç›®æ ‡å®ä½“:")
                            print(f"        ç›®æ ‡: {target_name} ({final_target_id}, ç‰ˆæœ¬æ•°: {final_target_versions})")
                            merge_names = [f"{self.storage.get_entity_by_id(sid).name} ({sid})" if self.storage.get_entity_by_id(sid) else sid for sid in final_source_ids]
                            print(f"        æºå®ä½“: {', '.join(merge_names)}")
                            print(f"        åŸå› : {combined_reason}")
                        
                        # æ‰§è¡Œåˆå¹¶ï¼ˆä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰sourceåˆ°targetï¼‰
                        merge_result = self.storage.merge_entity_ids(final_target_id, final_source_ids)
                        merge_result["reason"] = combined_reason
                        merge_result["target_versions"] = final_target_versions
                        
                        if verbose:
                            print(f"        ç»“æœ: æ›´æ–°äº† {merge_result.get('entities_updated', 0)} æ¡å®ä½“è®°å½•")
                        
                        # å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
                        self._handle_self_referential_relations_after_merge(final_target_id, verbose)
                        
                        # è®°å½•å·²åˆå¹¶çš„entity_id
                        for sid in final_source_ids:
                            merged_entity_ids.add(sid)
                            all_merged_in_this_round.add(sid)
                        
                        result["merge_details"].append(merge_result)
                        result["entities_merged"] += merge_result.get("entities_updated", 0)
                
                # ç«‹å³åˆ›å»ºå…³è”å…³ç³»ï¼ˆæ­¥éª¤4ï¼‰
                if alias_relations:
                    if verbose:
                        print(f"      åˆ›å»ºå…³è”å…³ç³»...")
                        if self.llm_threads > 1 and len(alias_relations) > 1:
                            print(f"      ä½¿ç”¨ {self.llm_threads} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç† {len(alias_relations)} ä¸ªå…³ç³»...")
                    
                    # æ„å»ºæœ‰æ•ˆçš„entity_idæ˜ å°„ï¼ˆç”¨äºéªŒè¯LLMè¿”å›çš„IDæ˜¯å¦æœ‰æ•ˆï¼‰
                    valid_entity_ids = {e["entity_id"] for e in entities_for_analysis}
                    entity_id_entity2_name = {e["entity_id"]: e["name"] for e in entities_for_analysis}
                    
                    # å‡†å¤‡æ‰€æœ‰éœ€è¦å¤„ç†çš„å…³ç³»ä¿¡æ¯
                    relations_to_process = []
                    
                    for alias_info in alias_relations:
                        entity1_id = alias_info.get("entity1_id")
                        entity2_id = alias_info.get("entity2_id")
                        entity1_name = alias_info.get("entity1_name", "")
                        entity2_name = alias_info.get("entity2_name", "")
                        # æ³¨æ„ï¼šç°åœ¨alias_infoä¸­ä¸å†åŒ…å«contentï¼Œéœ€è¦åœ¨åç»­æ­¥éª¤ä¸­ç”Ÿæˆ
                        
                        if verbose:
                            print(f"        å¤„ç†å…³ç³»: {entity1_name} ({entity1_id}) -> {entity2_name} ({entity2_id})")
                        
                        if not entity1_id or not entity2_id:
                            if verbose:
                                print(f"          è·³è¿‡ï¼šç¼ºå°‘entity_id (entity1: {entity1_id}, entity2: {entity2_id})")
                            continue
                        
                        # éªŒè¯entity_idæ˜¯å¦åœ¨ä¼ å…¥çš„å®ä½“åˆ—è¡¨ä¸­
                        if entity1_id not in valid_entity_ids:
                            if verbose:
                                print(f"          è­¦å‘Šï¼šentity1_id {entity1_id} ä¸åœ¨åˆ†æåˆ—è¡¨ä¸­ï¼Œå°è¯•é€šè¿‡åç§°æŸ¥æ‰¾...")
                            # å°è¯•é€šè¿‡åç§°æŸ¥æ‰¾å®ä½“
                            found_entity = None
                            for e in entities_for_analysis:
                                if e["name"] == entity1_name:
                                    found_entity = e
                                    break
                            if found_entity:
                                entity1_id = found_entity["entity_id"]
                                if verbose:
                                    print(f"            é€šè¿‡åç§°æ‰¾åˆ°å®ä½“: {entity1_name} -> {entity1_id}")
                            else:
                                if verbose:
                                    print(f"            æ— æ³•æ‰¾åˆ°å®ä½“: {entity1_name} ({entity1_id})")
                                continue
                        
                        if entity2_id not in valid_entity_ids:
                            if verbose:
                                print(f"          è­¦å‘Šï¼šentity2_id {entity2_id} ä¸åœ¨åˆ†æåˆ—è¡¨ä¸­ï¼Œå°è¯•é€šè¿‡åç§°æŸ¥æ‰¾...")
                            # å°è¯•é€šè¿‡åç§°æŸ¥æ‰¾å®ä½“
                            found_entity = None
                            for e in entities_for_analysis:
                                if e["name"] == entity2_name:
                                    found_entity = e
                                    break
                            if found_entity:
                                entity2_id = found_entity["entity_id"]
                                if verbose:
                                    print(f"            é€šè¿‡åç§°æ‰¾åˆ°å®ä½“: {entity2_name} -> {entity2_id}")
                            else:
                                if verbose:
                                    print(f"            æ— æ³•æ‰¾åˆ°å®ä½“: {entity2_name} ({entity2_id})")
                                continue
                        
                        # æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨æœ¬æ¬¡å¾ªç¯ä¸­è¢«åˆå¹¶ï¼ˆå¦‚æœè¢«åˆå¹¶ï¼Œéœ€è¦ä½¿ç”¨åˆå¹¶åçš„entity_idï¼‰
                        actual_entity1_id = entity1_id
                        actual_entity2_id = entity2_id
                        
                        # å¦‚æœentity1å®ä½“åœ¨æœ¬æ¬¡å¾ªç¯ä¸­è¢«åˆå¹¶ï¼Œä½¿ç”¨æœ€ç»ˆçš„target_id
                        if entity1_id in all_merged_in_this_round and final_target_id:
                            actual_entity1_id = final_target_id
                        
                        # å¦‚æœentity2å®ä½“åœ¨æœ¬æ¬¡å¾ªç¯ä¸­è¢«åˆå¹¶ï¼Œä½¿ç”¨æœ€ç»ˆçš„target_id
                        if entity2_id in all_merged_in_this_round and final_target_id:
                            actual_entity2_id = final_target_id
                        
                        # æ£€æŸ¥å®ä½“æ˜¯å¦åœ¨ä¹‹å‰çš„å¾ªç¯ä¸­å·²è¢«åˆå¹¶
                        # å¦‚æœentity_idåœ¨merged_entity_idsä¸­ï¼Œè¯´æ˜å·²ç»è¢«åˆå¹¶ï¼Œéœ€è¦æ‰¾åˆ°åˆå¹¶åçš„entity_id
                        if entity1_id in merged_entity_ids:
                            # ä»merge_detailsä¸­æŸ¥æ‰¾è¯¥å®ä½“è¢«åˆå¹¶åˆ°å“ªä¸ªtarget
                            found_target = None
                            for merge_detail in result["merge_details"]:
                                if entity1_id in merge_detail.get("merged_source_ids", []):
                                    found_target = merge_detail.get("target_entity_id")
                                    break
                            if found_target:
                                actual_entity1_id = found_target
                                if verbose:
                                    print(f"            æ³¨æ„ï¼šentity1å®ä½“ {entity1_name} ({entity1_id}) å·²è¢«åˆå¹¶åˆ° {found_target}")
                            else:
                                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥è¯¢æ•°æ®åº“ï¼ˆå¯èƒ½entity_idå·²ç»æ›´æ–°ï¼‰
                                entity1_db = self.storage.get_entity_by_id(entity1_id)
                                if entity1_db:
                                    actual_entity1_id = entity1_db.entity_id
                        
                        if entity2_id in merged_entity_ids:
                            # ä»merge_detailsä¸­æŸ¥æ‰¾è¯¥å®ä½“è¢«åˆå¹¶åˆ°å“ªä¸ªtarget
                            found_target = None
                            for merge_detail in result["merge_details"]:
                                if entity2_id in merge_detail.get("merged_source_ids", []):
                                    found_target = merge_detail.get("target_entity_id")
                                    break
                            if found_target:
                                actual_entity2_id = found_target
                                if verbose:
                                    print(f"            æ³¨æ„ï¼šentity2å®ä½“ {entity2_name} ({entity2_id}) å·²è¢«åˆå¹¶åˆ° {found_target}")
                            else:
                                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥è¯¢æ•°æ®åº“ï¼ˆå¯èƒ½entity_idå·²ç»æ›´æ–°ï¼‰
                                entity2_db = self.storage.get_entity_by_id(entity2_id)
                                if entity2_db:
                                    actual_entity2_id = entity2_db.entity_id
                        
                        # éªŒè¯æœ€ç»ˆçš„entity_idæ˜¯å¦æœ‰æ•ˆ
                        entity1_check = self.storage.get_entity_by_id(actual_entity1_id)
                        entity2_check = self.storage.get_entity_by_id(actual_entity2_id)
                        
                        if not entity1_check:
                            if verbose:
                                print(f"          é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°entity1å®ä½“ (entity_id: {actual_entity1_id}, name: {entity1_name})")
                            continue
                        
                        if not entity2_check:
                            if verbose:
                                print(f"          é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°entity2å®ä½“ (entity_id: {actual_entity2_id}, name: {entity2_name})")
                            continue
                        
                        # å¦‚æœåˆå¹¶åentity1å’Œentity2æ˜¯åŒä¸€ä¸ªå®ä½“ï¼Œè·³è¿‡åˆ›å»ºå…³ç³»
                        if actual_entity1_id == actual_entity2_id:
                            if verbose:
                                print(f"          è·³è¿‡ï¼šåˆå¹¶åentity1å’Œentity2æ˜¯åŒä¸€å®ä½“")
                            continue
                        
                        if verbose:
                            print(f"          å‡†å¤‡å¤„ç†å…³ç³»: {entity1_name} -> {entity2_name}")
                            if actual_entity1_id != entity1_id or actual_entity2_id != entity2_id:
                                print(f"            æ³¨æ„ï¼šä½¿ç”¨äº†åˆå¹¶åçš„entity_id (entity1: {entity1_id}->{actual_entity1_id}, entity2: {entity2_id}->{actual_entity2_id})")
                        
                        # æ”¶é›†å…³ç³»ä¿¡æ¯ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†
                        # ä»alias_infoä¸­è·å–åˆæ­¥çš„contentï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        preliminary_content = alias_info.get("content")
                        relations_to_process.append({
                            "entity1_id": entity1_id,
                            "entity2_id": entity2_id,
                            "actual_entity1_id": actual_entity1_id,
                            "actual_entity2_id": actual_entity2_id,
                            "entity1_name": entity1_name,
                            "entity2_name": entity2_name,
                            "content": preliminary_content  # åˆæ­¥çš„contentï¼Œç”¨äºé¢„åˆ¤æ–­
                        })
                
                # å¹¶è¡Œå¤„ç†å…³ç³»
                if verbose:
                    print(f"      å‡†å¤‡å¤„ç† {len(relations_to_process)} ä¸ªå…³ç³»ï¼Œllm_threads={self.llm_threads}")
                if self.llm_threads > 1 and len(relations_to_process) > 1:
                    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
                    if verbose:
                        print(f"      ä½¿ç”¨ {self.llm_threads} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç† {len(relations_to_process)} ä¸ªå…³ç³»...")
                    with ThreadPoolExecutor(max_workers=self.llm_threads) as executor:
                        # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆå¤šçº¿ç¨‹æ¨¡å¼ä¸‹ä¸æ˜¾ç¤ºæ¯ä¸ªå…³ç³»çš„è¯¦ç»†ä¿¡æ¯ï¼‰
                        future_to_relation = {
                            executor.submit(
                                self._process_single_alias_relation,
                                rel_info,
                                False  # å¤šçº¿ç¨‹æ¨¡å¼ä¸‹ä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                            ): rel_info
                            for rel_info in relations_to_process
                        }
                        
                        # æ”¶é›†ç»“æœ
                        for future in as_completed(future_to_relation):
                            rel_info = future_to_relation[future]
                            try:
                                result_data = future.result()
                                if result_data:
                                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                                    if result_data.get("is_new"):
                                        result["alias_relations_created"] += 1
                                    elif result_data.get("is_updated"):
                                        result["alias_relations_updated"] += 1
                                    result["alias_details"].append(result_data)
                            except Exception as e:
                                if verbose:
                                    print(f"      å¤„ç†å…³ç³» {rel_info['entity1_name']} -> {rel_info['entity2_name']} å¤±è´¥: {e}")
                else:
                    # ä¸²è¡Œå¤„ç†
                    if verbose:
                        if self.llm_threads <= 1:
                            print(f"      ä¸²è¡Œå¤„ç† {len(relations_to_process)} ä¸ªå…³ç³»ï¼ˆllm_threads={self.llm_threads}ï¼Œæœªå¯ç”¨å¤šçº¿ç¨‹ï¼‰")
                        elif len(relations_to_process) <= 1:
                            print(f"      ä¸²è¡Œå¤„ç† {len(relations_to_process)} ä¸ªå…³ç³»ï¼ˆå…³ç³»æ•°é‡ <= 1ï¼Œæ— éœ€å¹¶è¡Œï¼‰")
                    for rel_info in relations_to_process:
                        try:
                            result_data = self._process_single_alias_relation(rel_info, verbose)
                            if result_data:
                                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                                if result_data.get("is_new"):
                                    result["alias_relations_created"] += 1
                                elif result_data.get("is_updated"):
                                    result["alias_relations_updated"] += 1
                                result["alias_details"].append(result_data)
                        except Exception as e:
                            if verbose:
                                print(f"      å¤„ç†å…³ç³» {rel_info['entity1_name']} -> {rel_info['entity2_name']} å¤±è´¥: {e}")
    
    def _consolidate_knowledge_graph_parallel(self, verbose: bool = True, 
                                              similarity_threshold: float = 0.6,
                                              max_candidates: int = 5,
                                              batch_candidates: Optional[int] = None,
                                              content_snippet_length: int = 64) -> dict:
        """
        å¤šçº¿ç¨‹å¹¶è¡Œç‰ˆæœ¬çš„çŸ¥è¯†å›¾è°±æ•´ç†
        
        é€šè¿‡é¢„æ’é™¤å…³è”å®ä½“æ¥é¿å…å¹¶è¡Œå¤„ç†æ—¶çš„å†²çªï¼š
        1. é¢„å…ˆæœç´¢æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“
        2. è°ƒåº¦å™¨é€‰æ‹©ä¸å†²çªçš„å®ä½“å¹¶è¡Œå¤„ç†
        3. çº¿ç¨‹å®Œæˆåï¼Œé‡Šæ”¾é”å®šçš„å®ä½“ï¼Œæ›´æ–°åˆå¹¶çŠ¶æ€
        
        Args:
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            similarity_threshold: ç›¸ä¼¼åº¦æœç´¢é˜ˆå€¼
            max_candidates: æ¯æ¬¡æœç´¢è¿”å›çš„æœ€å¤§å€™é€‰å®ä½“æ•°
            content_snippet_length: ä¼ å…¥LLMçš„å®ä½“contentæœ€å¤§é•¿åº¦
        
        Returns:
            æ•´ç†ç»“æœç»Ÿè®¡
        """
        import threading
        from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
        from queue import Queue
        
        if verbose:
            print("=" * 60)
            print(f"å¼€å§‹çŸ¥è¯†å›¾è°±æ•´ç†ï¼ˆå¤šçº¿ç¨‹æ¨¡å¼ï¼Œ{self.llm_threads}ä¸ªçº¿ç¨‹ï¼‰...")
            print("=" * 60)
        
        # ç»“æœç»Ÿè®¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        result = {
            "entities_analyzed": 0,
            "entities_merged": 0,
            "alias_relations_created": 0,
            "alias_relations_updated": 0,
            "merge_details": [],
            "alias_details": []
        }
        result_lock = threading.Lock()
        
        # æ­¥éª¤1ï¼šè·å–æ‰€æœ‰å®ä½“
        if verbose:
            print(f"\næ­¥éª¤1: è·å–æ‰€æœ‰å®ä½“...")
        
        all_entities = self.storage.get_all_entities()
        
        if not all_entities:
            if verbose:
                print("  çŸ¥è¯†åº“ä¸­æ²¡æœ‰å®ä½“ã€‚")
            return result
        
        # æŒ‰ç‰ˆæœ¬æ•°é‡ä»å¤§åˆ°å°æ’åº
        entity_ids = [entity.entity_id for entity in all_entities]
        version_counts = self.storage.get_entity_version_counts(entity_ids)
        all_entities.sort(key=lambda e: version_counts.get(e.entity_id, 0), reverse=True)
        
        initial_entity_count = len(all_entities)
        if verbose:
            print(f"  æ•´ç†å‰å…±æœ‰ {initial_entity_count} ä¸ªå®ä½“")
        
        # æ­¥éª¤1.5ï¼šå…ˆæŒ‰åç§°å®Œå…¨åŒ¹é…è¿›è¡Œæ•´ç†
        if verbose:
            print(f"\næ­¥éª¤1.5: æŒ‰åç§°å®Œå…¨åŒ¹é…è¿›è¡Œåˆæ­¥æ•´ç†...")
        
        # è®°å½•å·²åˆå¹¶çš„å®ä½“IDï¼ˆç”¨äºåç»­embeddingæœç´¢æ—¶æ’é™¤ï¼‰
        merged_entity_ids = set()
        # è®°å½•åˆå¹¶æ˜ å°„ï¼šsource_entity_id -> target_entity_id
        merge_mapping = {}
        
        # æ„å»ºåç§°åˆ°å®ä½“åˆ—è¡¨çš„æ˜ å°„
        name_to_entities = {}
        for entity in all_entities:
            name = entity.name
            if name not in name_to_entities:
                name_to_entities[name] = []
            name_to_entities[name].append(entity)
        
        # å¯¹æ¯ä¸ªåç§°ç»„å†…çš„å®ä½“æŒ‰ç‰ˆæœ¬æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
        for name in name_to_entities:
            name_to_entities[name].sort(
                key=lambda e: version_counts.get(e.entity_id, 0), 
                reverse=True
            )
        
        # æŒ‰ç…§æ¯ä¸ªåç§°ç»„ä¸­å®ä½“çš„æœ€å¤§ç‰ˆæœ¬æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰ï¼Œç„¶åæŒ‰é¡ºåºå¤„ç†
        name_groups_sorted = sorted(
            name_to_entities.items(),
            key=lambda item: max(
                (version_counts.get(e.entity_id, 0) for e in item[1]),
                default=0
            ),
            reverse=True
        )
        
        # å¤„ç†åç§°å®Œå…¨ä¸€è‡´çš„å®ä½“ç»„
        name_match_count = 0
        for name, entities_with_same_name in name_groups_sorted:
            # åªå¤„ç†æœ‰å¤šä¸ªå®ä½“çš„åç§°ç»„
            if len(entities_with_same_name) <= 1:
                continue
            
            name_match_count += 1
            if verbose:
                print(f"  å‘ç°åç§°å®Œå…¨ä¸€è‡´çš„å®ä½“ç»„: {name} (å…± {len(entities_with_same_name)} ä¸ªå®ä½“)")
            
            # å‡†å¤‡å®ä½“ä¿¡æ¯ç”¨äºLLMåˆ¤æ–­
            entities_info = []
            for entity in entities_with_same_name:
                # è·³è¿‡å·²åˆå¹¶çš„å®ä½“
                if entity.entity_id in merged_entity_ids:
                    continue
                
                version_count = version_counts.get(entity.entity_id, 0)
                entities_info.append({
                    "entity_id": entity.entity_id,
                    "name": entity.name,
                    "content": entity.content,
                    "version_count": version_count
                })
            
            # å¦‚æœè¿‡æ»¤ååªå‰©ä¸€ä¸ªæˆ–æ²¡æœ‰å®ä½“ï¼Œè·³è¿‡
            if len(entities_info) <= 1:
                continue
            
            # è·å–è®°å¿†ä¸Šä¸‹æ–‡
            memory_contexts = {}
            for entity in entities_with_same_name:
                if entity.entity_id in merged_entity_ids:
                    continue
                cache_text = self.storage.get_memory_cache_text(entity.memory_cache_id)
                if cache_text:
                    memory_contexts[entity.entity_id] = cache_text
            
            # æ£€æŸ¥å®ä½“å¯¹ä¹‹é—´æ˜¯å¦å·²æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“åˆ™ç›´æ¥åˆå¹¶
            entity_ids_for_check = [info['entity_id'] for info in entities_info]
            existing_relations_between = self._check_and_merge_entities_from_relations(
                entity_ids_for_check,
                entities_info,
                version_counts,
                merged_entity_ids,
                merge_mapping,
                result,
                verbose
            )
            
            if verbose and existing_relations_between:
                print(f"    å‘ç° {len(existing_relations_between)} å¯¹å®ä½“ä¹‹é—´å·²æœ‰å…³ç³»ï¼ˆéåŒä¸€å®ä½“å…³ç³»ï¼‰ï¼Œè¿™äº›å®ä½“å¯¹ä¸ä¼šè¢«åˆå¹¶")
            
            # è°ƒç”¨LLMåˆ†æï¼šåˆ¤æ–­æ˜¯åˆå¹¶è¿˜æ˜¯å…³è”å…³ç³»
            analysis_result = self.llm_client.analyze_entity_duplicates(
                entities_info,
                memory_contexts,
                content_snippet_length=content_snippet_length,
                existing_relations_between_entities=existing_relations_between
            )
            
            if "error" in analysis_result:
                if verbose:
                    print(f"    åˆ†æå¤±è´¥ï¼Œè·³è¿‡è¯¥ç»„")
                continue
            
            # å¤„ç†åˆå¹¶ï¼ˆè¿‡æ»¤æ‰å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼‰
            merge_groups = analysis_result.get("merge_groups", [])
            for merge_group in merge_groups:
                target_entity_id = merge_group.get("target_entity_id")
                source_entity_ids = merge_group.get("source_entity_ids", [])
                reason = merge_group.get("reason", "")
                
                if not target_entity_id or not source_entity_ids:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                if any(sid in merged_entity_ids for sid in source_entity_ids):
                    continue
                
                # ä¸å†è¿‡æ»¤å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼Œè®©LLMåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                # å³ä½¿æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œä¹Ÿåº”è¯¥åˆå¹¶
                
                # æ‰§è¡Œåˆå¹¶
                merge_result = self.storage.merge_entity_ids(target_entity_id, source_entity_ids)
                merge_result["reason"] = reason
                
                if verbose:
                    target_name = next((e.name for e in entities_with_same_name if e.entity_id == target_entity_id), target_entity_id)
                    print(f"    åˆå¹¶å®ä½“: {target_name} ({target_entity_id}) <- {len(source_entity_ids)} ä¸ªæºå®ä½“")
                    print(f"      åŸå› : {reason}")
                
                # å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
                self._handle_self_referential_relations_after_merge(target_entity_id, verbose)
                
                # è®°å½•å·²åˆå¹¶çš„å®ä½“å’Œåˆå¹¶æ˜ å°„
                for sid in source_entity_ids:
                    merged_entity_ids.add(sid)
                    merge_mapping[sid] = target_entity_id
                
                # æ›´æ–°ç»“æœç»Ÿè®¡
                result["merge_details"].append(merge_result)
                result["entities_merged"] += merge_result.get("entities_updated", 0)
            
            # å¤„ç†å…³ç³»ï¼ˆåˆ«åå…³ç³»ï¼‰
            alias_relations = analysis_result.get("alias_relations", [])
            for alias_info in alias_relations:
                entity1_id = alias_info.get("entity1_id")
                entity2_id = alias_info.get("entity2_id")
                entity1_name = alias_info.get("entity1_name", "")
                entity2_name = alias_info.get("entity2_name", "")
                preliminary_content = alias_info.get("content")
                
                if not entity1_id or not entity2_id:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶ï¼ˆå¦‚æœå·²åˆå¹¶ï¼Œéœ€è¦æ‰¾åˆ°åˆå¹¶åçš„å®é™…IDï¼‰
                actual_entity1_id = merge_mapping.get(entity1_id, entity1_id)
                actual_entity2_id = merge_mapping.get(entity2_id, entity2_id)
                
                # å¦‚æœå®ä½“å·²è¢«åˆå¹¶ï¼Œè·³è¿‡ï¼ˆå› ä¸ºåˆå¹¶åçš„å®ä½“å¯èƒ½ä¸åœ¨å½“å‰åç§°ç»„ä¸­ï¼‰
                if entity1_id in merged_entity_ids or entity2_id in merged_entity_ids:
                    if verbose:
                        print(f"    è·³è¿‡å…³ç³»ï¼ˆå®ä½“å·²åˆå¹¶ï¼‰: {entity1_name} -> {entity2_name}")
                    continue
                
                # å¤„ç†å…³ç³»
                rel_info = {
                    "entity1_id": entity1_id,
                    "entity2_id": entity2_id,
                    "actual_entity1_id": actual_entity1_id,
                    "actual_entity2_id": actual_entity2_id,
                    "entity1_name": entity1_name,
                    "entity2_name": entity2_name,
                    "content": preliminary_content
                }
                
                rel_result = self._process_single_alias_relation(rel_info, verbose=False)
                if rel_result:
                    result["alias_details"].append(rel_result)
                    if rel_result.get("is_new"):
                        result["alias_relations_created"] += 1
                    elif rel_result.get("is_updated"):
                        result["alias_relations_updated"] += 1
        
        if verbose:
            print(f"  åç§°åŒ¹é…å®Œæˆï¼Œå¤„ç†äº† {name_match_count} ä¸ªåç§°ç»„ï¼Œåˆå¹¶äº† {len(merged_entity_ids)} ä¸ªå®ä½“")
        
        # æ­¥éª¤2ï¼šä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼ä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“
        if verbose:
            print(f"\næ­¥éª¤2: ä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼é¢„æœç´¢æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“...")
            print(f"  ä½¿ç”¨å¤šç§æ£€ç´¢æ¨¡å¼ï¼šname_only(embedding) + name_and_content(embedding) + name_only(text/jaccard)")
        
        # ä½¿ç”¨æ··åˆæ£€ç´¢æ–¹å¼ä¸€æ¬¡æ€§æ‰¾åˆ°æ‰€æœ‰å®ä½“çš„å…³è”å®ä½“
        entity_to_candidates = self.storage.find_related_entities_by_embedding(
            similarity_threshold=similarity_threshold,
            max_candidates=max_candidates,
            use_mixed_search=True,  # å¯ç”¨æ··åˆæ£€ç´¢
            content_snippet_length=content_snippet_length
        )
        
        # è¿‡æ»¤æ‰å·²åˆå¹¶çš„å®ä½“ï¼ˆåœ¨å€™é€‰åˆ—è¡¨ä¸­æ’é™¤ï¼‰
        for entity_id in list(entity_to_candidates.keys()):
            # å¦‚æœå½“å‰å®ä½“å·²åˆå¹¶ï¼Œä»å€™é€‰åˆ—è¡¨ä¸­ç§»é™¤
            if entity_id in merged_entity_ids:
                del entity_to_candidates[entity_id]
                continue
            
            # ä»å€™é€‰åˆ—è¡¨ä¸­æ’é™¤å·²åˆå¹¶çš„å®ä½“
            candidates = entity_to_candidates[entity_id]
            entity_to_candidates[entity_id] = candidates - merged_entity_ids
        
        if verbose:
            total_candidates = sum(len(candidates) for candidates in entity_to_candidates.values())
            print(f"  é¢„æœç´¢å®Œæˆï¼Œå…± {len(entity_to_candidates)} ä¸ªå®ä½“ï¼Œæ‰¾åˆ° {total_candidates} ä¸ªå…³è”å®ä½“ï¼ˆå·²æ’é™¤ {len(merged_entity_ids)} ä¸ªå·²åˆå¹¶å®ä½“ï¼‰")
        
        # æ­¥éª¤3ï¼šå¹¶è¡Œå¤„ç†å®ä½“
        if verbose:
            print(f"\næ­¥éª¤3: å¹¶è¡Œå¤„ç†å®ä½“ï¼ˆ{self.llm_threads}ä¸ªçº¿ç¨‹ï¼‰...")
        
        # å…±äº«çŠ¶æ€ï¼ˆéœ€è¦åŠ é”ï¼‰
        # merged_entity_ids å·²ç»åœ¨æ­¥éª¤1.5ä¸­åˆå§‹åŒ–ï¼Œè¿™é‡Œåªéœ€è¦åˆ›å»ºé”
        merged_ids_lock = threading.Lock()
        
        in_progress_ids = set()  # æ­£åœ¨å¤„ç†ä¸­çš„å®ä½“IDï¼ˆåŒ…æ‹¬å…³è”å®ä½“ï¼‰
        in_progress_lock = threading.Lock()
        
        processed_pairs = set()
        processed_pairs_lock = threading.Lock()
        
        # å¾…å¤„ç†å®ä½“åˆ—è¡¨
        pending_entities = list(all_entities)
        pending_lock = threading.Lock()
        
        # ç”¨äºç´¯ç§¯æ‰€æœ‰åˆ†æè¿‡çš„å®ä½“ä¿¡æ¯
        all_analyzed_entities_text = []
        analyzed_text_lock = threading.Lock()
        
        # è®¡æ•°å™¨
        processed_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        count_lock = threading.Lock()
        
        def get_next_entity():
            """
            è·å–ä¸‹ä¸€ä¸ªå¯ä»¥å¤„ç†çš„å®ä½“ï¼ˆä¸ä¸æ­£åœ¨å¤„ç†çš„å®ä½“å†²çªï¼‰
            è¿”å›: (entity, candidate_ids) æˆ– (None, None)
            """
            with pending_lock:
                for i, entity in enumerate(pending_entities):
                    # æ£€æŸ¥æ˜¯å¦å·²åˆå¹¶
                    with merged_ids_lock:
                        if entity.entity_id in merged_entity_ids:
                            pending_entities.pop(i)
                            continue
                    
                    # è·å–å…³è”å®ä½“
                    candidates = entity_to_candidates.get(entity.entity_id, set())
                    
                    # è¿‡æ»¤æ‰å·²åˆå¹¶çš„å…³è”å®ä½“
                    with merged_ids_lock:
                        candidates = candidates - merged_entity_ids
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸æ­£åœ¨å¤„ç†çš„å®ä½“å†²çª
                    all_ids = {entity.entity_id} | candidates
                    with in_progress_lock:
                        if all_ids & in_progress_ids:
                            continue  # æœ‰å†²çªï¼Œè·³è¿‡
                        
                        # æ ‡è®°ä¸ºæ­£åœ¨å¤„ç†
                        in_progress_ids.update(all_ids)
                    
                    # æ‰¾åˆ°äº†å¯ä»¥å¤„ç†çš„å®ä½“
                    pending_entities.pop(i)
                    return entity, candidates
            
            return None, None
        
        def release_entity(entity_id, candidate_ids):
            """é‡Šæ”¾å®ä½“çš„å¤„ç†æƒ"""
            all_ids = {entity_id} | candidate_ids
            with in_progress_lock:
                in_progress_ids.difference_update(all_ids)
        
        def process_entity_task(entity, candidate_ids):
            """
            å¤„ç†å•ä¸ªå®ä½“åŠå…¶å…³è”å®ä½“
            è¿”å›å¤„ç†ç»“æœ
            """
            task_result = {
                "entities_analyzed": 0,
                "entities_merged": 0,
                "alias_relations_created": 0,
                "alias_relations_updated": 0,
                "merge_details": [],
                "alias_details": [],
                "merged_ids": set(),
                "analyzed_text": ""
            }
            
            try:
                # è¿‡æ»¤å·²å¤„ç†çš„é…å¯¹
                with processed_pairs_lock:
                    filtered_candidates = {
                        cid for cid in candidate_ids
                        if (min(entity.entity_id, cid), max(entity.entity_id, cid)) not in processed_pairs
                    }
                    # è®°å½•é…å¯¹
                    for cid in filtered_candidates:
                        processed_pairs.add((min(entity.entity_id, cid), max(entity.entity_id, cid)))
                
                if not filtered_candidates:
                    return task_result
                
                # è·å–å€™é€‰å®ä½“çš„å®Œæ•´ä¿¡æ¯
                candidates_info = []
                for cid in filtered_candidates:
                    candidate_entity = self.storage.get_entity_by_id(cid)
                    if candidate_entity:
                        version_count = self.storage.get_entity_version_count(cid)
                        candidates_info.append({
                            "entity_id": cid,
                            "name": candidate_entity.name,
                            "content": candidate_entity.content,
                            "version_count": version_count
                        })
                
                if not candidates_info:
                    return task_result
                
                # å‡†å¤‡å½“å‰å®ä½“ä¿¡æ¯
                current_version_count = self.storage.get_entity_version_count(entity.entity_id)
                current_entity_info = {
                    "entity_id": entity.entity_id,
                    "name": entity.name,
                    "content": entity.content,
                    "version_count": current_version_count
                }
                
                entities_for_analysis = [current_entity_info] + candidates_info
                
                # è·å–è®°å¿†ä¸Šä¸‹æ–‡
                memory_contexts = {}
                cache_text = self.storage.get_memory_cache_text(entity.memory_cache_id)
                if cache_text:
                    memory_contexts[entity.entity_id] = cache_text
                
                for info in candidates_info:
                    candidate_entity = self.storage.get_entity_by_id(info["entity_id"])
                    if candidate_entity:
                        c_text = self.storage.get_memory_cache_text(candidate_entity.memory_cache_id)
                        if c_text:
                            memory_contexts[info["entity_id"]] = c_text
                
                # æ£€æŸ¥å®ä½“å¯¹ä¹‹é—´æ˜¯å¦å·²æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“åˆ™ç›´æ¥åˆå¹¶
                analysis_entity_ids = [info['entity_id'] for info in entities_for_analysis]
                existing_relations_between = self._check_and_merge_entities_from_relations(
                    analysis_entity_ids,
                    entities_for_analysis,
                    version_counts,
                    merged_entity_ids,
                    merge_mapping,
                    result,
                    verbose
                )
                
                # è°ƒç”¨LLMåˆ†æ
                analysis_result = self.llm_client.analyze_entity_duplicates(
                    entities_for_analysis,
                    memory_contexts,
                    content_snippet_length=content_snippet_length,
                    existing_relations_between_entities=existing_relations_between
                )
                
                if "error" in analysis_result:
                    return task_result
                
                task_result["entities_analyzed"] = 1
                task_result["analyzed_text"] = self._build_entity_list_text(entities_for_analysis)
                
                # å¤„ç†åˆå¹¶ï¼ˆè¿‡æ»¤æ‰å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼‰
                merge_groups = analysis_result.get("merge_groups", [])
                alias_relations = analysis_result.get("alias_relations", [])
                
                # æ‰§è¡Œåˆå¹¶æ“ä½œ
                for merge_group in merge_groups:
                    target_entity_id = merge_group.get("target_entity_id")
                    source_entity_ids = merge_group.get("source_entity_ids", [])
                    reason = merge_group.get("reason", "")
                    
                    if not target_entity_id or not source_entity_ids:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                    with merged_ids_lock:
                        if any(sid in merged_entity_ids for sid in source_entity_ids):
                            continue
                    
                    # ä¸å†è¿‡æ»¤å·²æœ‰å…³ç³»çš„å®ä½“å¯¹ï¼Œè®©LLMåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                    # å³ä½¿æœ‰å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œä¹Ÿåº”è¯¥åˆå¹¶
                    
                    # æ‰§è¡Œåˆå¹¶
                    merge_result = self.storage.merge_entity_ids(target_entity_id, source_entity_ids)
                    merge_result["reason"] = reason
                    
                    # å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
                    self._handle_self_referential_relations_after_merge(target_entity_id, verbose=False)
                    
                    task_result["merge_details"].append(merge_result)
                    task_result["entities_merged"] += merge_result.get("entities_updated", 0)
                    
                    # è®°å½•å·²åˆå¹¶çš„å®ä½“
                    for sid in source_entity_ids:
                        task_result["merged_ids"].add(sid)
                
                # å¤„ç†å…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼Œåªè®°å½•éœ€è¦åˆ›å»ºçš„å…³ç³»ï¼Œåç»­ç»Ÿä¸€å¤„ç†ï¼‰
                for alias_info in alias_relations:
                    entity1_id = alias_info.get("entity1_id")
                    entity2_id = alias_info.get("entity2_id")
                    entity1_name = alias_info.get("entity1_name", "")
                    entity2_name = alias_info.get("entity2_name", "")
                    preliminary_content = alias_info.get("content")
                    
                    if not entity1_id or not entity2_id:
                        continue
                    
                    # å¤„ç†å…³ç³»
                    rel_info = {
                        "entity1_id": entity1_id,
                        "entity2_id": entity2_id,
                        "actual_entity1_id": entity1_id,
                        "actual_entity2_id": entity2_id,
                        "entity1_name": entity1_name,
                        "entity2_name": entity2_name,
                        "content": preliminary_content
                    }
                    
                    rel_result = self._process_single_alias_relation(rel_info, verbose=False)
                    if rel_result:
                        task_result["alias_details"].append(rel_result)
                        if rel_result.get("is_new"):
                            task_result["alias_relations_created"] += 1
                        elif rel_result.get("is_updated"):
                            task_result["alias_relations_updated"] += 1
                
                return task_result
                
            except Exception as e:
                if verbose:
                    print(f"    å¤„ç†å®ä½“ {entity.name} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return task_result
        
        # ä¸»è°ƒåº¦å¾ªç¯
        with ThreadPoolExecutor(max_workers=self.llm_threads) as executor:
            futures = {}
            
            while True:
                # å°è¯•æäº¤æ–°ä»»åŠ¡ï¼ˆç›´åˆ°è¾¾åˆ°çº¿ç¨‹æ•°æˆ–æ²¡æœ‰å¯ç”¨å®ä½“ï¼‰
                while len(futures) < self.llm_threads:
                    entity, candidates = get_next_entity()
                    if entity is None:
                        break
                    
                    future = executor.submit(process_entity_task, entity, candidates)
                    futures[future] = (entity, candidates)
                    
                    with count_lock:
                        processed_count[0] += 1
                        if verbose:
                            print(f"\n  [{processed_count[0]}/{initial_entity_count}] å¼€å§‹å¤„ç†: {entity.name}")
                
                # å¦‚æœæ²¡æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ä¸”æ²¡æœ‰å¾…å¤„ç†çš„å®ä½“ï¼Œé€€å‡º
                if not futures:
                    with pending_lock:
                        if not pending_entities:
                            break
                        # è¿˜æœ‰å¾…å¤„ç†çš„å®ä½“ä½†éƒ½åœ¨å†²çªä¸­ï¼Œç­‰å¾…ä¸€ä¸‹
                    import time
                    time.sleep(0.1)
                    continue
                
                # ç­‰å¾…ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                done, _ = wait(futures.keys(), return_when=FIRST_COMPLETED)
                
                for future in done:
                    entity, candidates = futures.pop(future)
                    
                    try:
                        task_result = future.result()
                        
                        # æ›´æ–°å…¨å±€ç»“æœï¼ˆåŠ é”ï¼‰
                        with result_lock:
                            result["entities_analyzed"] += task_result["entities_analyzed"]
                            result["entities_merged"] += task_result["entities_merged"]
                            result["alias_relations_created"] += task_result["alias_relations_created"]
                            result["alias_relations_updated"] += task_result["alias_relations_updated"]
                            result["merge_details"].extend(task_result["merge_details"])
                            result["alias_details"].extend(task_result["alias_details"])
                        
                        # æ›´æ–°åˆå¹¶çŠ¶æ€
                        with merged_ids_lock:
                            merged_entity_ids.update(task_result["merged_ids"])
                        
                        # ç´¯ç§¯åˆ†ææ–‡æœ¬
                        if task_result["analyzed_text"]:
                            with analyzed_text_lock:
                                all_analyzed_entities_text.append(
                                    f"\n\n{'='*80}\nåˆ†æå®ä½“: {entity.name}\n{'='*80}\n"
                                )
                                all_analyzed_entities_text.append(task_result["analyzed_text"])
                        
                        if verbose and task_result["entities_analyzed"] > 0:
                            print(f"    å®Œæˆ: {entity.name} "
                                  f"(åˆå¹¶: {task_result['entities_merged']}, "
                                  f"æ–°å»ºå…³ç³»: {task_result['alias_relations_created']}, "
                                  f"æ›´æ–°å…³ç³»: {task_result['alias_relations_updated']})")
                    
                    finally:
                        # é‡Šæ”¾å¤„ç†æƒ
                        release_entity(entity.entity_id, candidates)
        
        # è°ƒç”¨æ”¶å°¾å·¥ä½œ
        self._finalize_consolidation(result, all_analyzed_entities_text, verbose)
        
        # è·å–æ•´ç†åçš„å®ä½“æ€»æ•°
        final_entities = self.storage.get_all_entities()
        final_entity_count = len(final_entities) if final_entities else 0
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡æ€»ç»“
        if verbose:
            print("\n" + "=" * 60)
            print("çŸ¥è¯†å›¾è°±æ•´ç†å®Œæˆï¼ï¼ˆå¤šçº¿ç¨‹æ¨¡å¼ï¼‰")
            print("=" * 60)
            print(f"ğŸ“Š å®ä½“ç»Ÿè®¡:")
            print(f"  - æ•´ç†å‰å®ä½“æ•°: {initial_entity_count}")
            print(f"  - æ•´ç†åå®ä½“æ•°: {final_entity_count}")
            print(f"  - å‡å°‘çš„å®ä½“æ•°: {initial_entity_count - final_entity_count}")
            print(f"")
            print(f"ğŸ“ˆ æ•´ç†æ“ä½œç»Ÿè®¡:")
            print(f"  - åˆ†æçš„å®ä½“æ•°: {result['entities_analyzed']}")
            print(f"  - åˆå¹¶çš„å®ä½“è®°å½•æ•°: {result['entities_merged']}")
            print(f"")
            print(f"ğŸ”— å…³ç³»è¾¹ç»Ÿè®¡:")
            print(f"  - æ–°å»ºçš„å…³ç³»è¾¹æ•°: {result['alias_relations_created']}")
            print(f"  - æ›´æ–°çš„å…³ç³»è¾¹æ•°: {result['alias_relations_updated']}")
            print(f"  - æ€»å¤„ç†çš„å…³ç³»è¾¹æ•°: {result['alias_relations_created'] + result['alias_relations_updated']}")
            print("=" * 60)
        
        return result
    
    def _get_existing_relations_between_entities(self, entity_ids: List[str]) -> Dict[str, List[Dict]]:
        """
        æ£€æŸ¥ä¸€ç»„å®ä½“ä¹‹é—´ä¸¤ä¸¤æ˜¯å¦å­˜åœ¨å·²æœ‰å…³ç³»
        
        Args:
            entity_ids: å®ä½“IDåˆ—è¡¨
        
        Returns:
            å·²æœ‰å…³ç³»å­—å…¸ï¼Œkeyä¸º "entity1_id|entity2_id" æ ¼å¼ï¼ˆæŒ‰å­—æ¯åºæ’åºï¼‰ï¼Œ
            valueä¸ºè¯¥å®ä½“å¯¹ä¹‹é—´çš„å…³ç³»åˆ—è¡¨ï¼Œæ¯ä¸ªå…³ç³»åŒ…å«:
                - relation_id: å…³ç³»ID
                - content: å…³ç³»æè¿°
        """
        existing_relations = {}
        
        # éå†æ‰€æœ‰å®ä½“å¯¹
        for i, entity1_id in enumerate(entity_ids):
            for entity2_id in entity_ids[i+1:]:
                # æ£€æŸ¥ä¸¤ä¸ªå®ä½“ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³ç³»
                relations = self.storage.get_relations_by_entities(entity1_id, entity2_id)
                
                if relations:
                    # æŒ‰å­—æ¯åºæ’åºå®ä½“IDä½œä¸ºkey
                    sorted_ids = sorted([entity1_id, entity2_id])
                    pair_key = f"{sorted_ids[0]}|{sorted_ids[1]}"
                    
                    # æŒ‰relation_idåˆ†ç»„ï¼Œæ¯ä¸ªrelation_idåªä¿ç•™æœ€æ–°ç‰ˆæœ¬
                    relation_dict = {}
                    for rel in relations:
                        if rel.relation_id not in relation_dict:
                            relation_dict[rel.relation_id] = rel
                        else:
                            if rel.physical_time > relation_dict[rel.relation_id].physical_time:
                                relation_dict[rel.relation_id] = rel
                    
                    # æå–å…³ç³»ä¿¡æ¯
                    existing_relations[pair_key] = [
                        {
                            'relation_id': r.relation_id,
                            'content': r.content
                        }
                        for r in relation_dict.values()
                    ]
        
        return existing_relations
    
    def _is_relation_indicating_same_entity(self, relation_content: str) -> bool:
        """
        åˆ¤æ–­å…³ç³»æ˜¯å¦è¡¨ç¤ºä¸¤ä¸ªå®ä½“æ˜¯åŒä¸€å®ä½“
        
        Args:
            relation_content: å…³ç³»çš„contentæè¿°
        
        Returns:
            å¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œè¿”å›Trueï¼›å¦åˆ™è¿”å›False
        """
        if not relation_content:
            return False
        
        content_lower = relation_content.lower()
        
        # å…³é”®è¯åˆ—è¡¨ï¼šè¡¨ç¤ºåŒä¸€å®ä½“çš„å…³ç³»æè¿°
        same_entity_keywords = [
            "åŒä¸€å®ä½“", "åŒä¸€ä¸ª", "åŒä¸€äºº", "åŒä¸€ç‰©", "åŒä¸€å¯¹è±¡",
            "åˆ«å", "åˆ«ç§°", "åˆç§°", "ä¹Ÿå«", "äº¦ç§°",
            "æ˜¯", "å°±æ˜¯", "å³æ˜¯", "ç­‰äº", "ç­‰åŒäº",
            "æŒ‡", "æŒ‡çš„æ˜¯", "æŒ‡å‘", "è¡¨ç¤º",
            "ç®€ç§°", "å…¨ç§°", "æ˜µç§°", "ç»°å·", "å¤–å·",
            "æœ¬å", "åŸå", "çœŸå", "å®å"
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword in same_entity_keywords:
            if keyword in content_lower:
                return True
        
        return False
    
    def _check_and_merge_entities_from_relations(self, entity_ids: List[str], 
                                                  entities_info: List[Dict],
                                                  version_counts: Dict[str, int],
                                                  merged_entity_ids: set,
                                                  merge_mapping: Dict[str, str],
                                                  result: Dict,
                                                  verbose: bool = True) -> Dict[str, List[Dict]]:
        """
        æ£€æŸ¥å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚æœå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œåˆ™ç›´æ¥åˆå¹¶
        
        Args:
            entity_ids: å®ä½“IDåˆ—è¡¨
            entities_info: å®ä½“ä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«nameç­‰ï¼‰
            version_counts: å®ä½“ç‰ˆæœ¬æ•°ç»Ÿè®¡
            merged_entity_ids: å·²åˆå¹¶çš„å®ä½“IDé›†åˆ
            merge_mapping: åˆå¹¶æ˜ å°„å­—å…¸
            result: ç»“æœç»Ÿè®¡å­—å…¸
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            è¿‡æ»¤åçš„å·²æœ‰å…³ç³»å­—å…¸ï¼ˆå·²æ’é™¤è¡¨ç¤ºåŒä¸€å®ä½“çš„å…³ç³»ï¼‰
        """
        existing_relations_between = self._get_existing_relations_between_entities(entity_ids)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œå¦‚æœæœ‰åˆ™ç›´æ¥åˆå¹¶
        entities_to_merge_from_relations = []
        
        for pair_key, relations in existing_relations_between.items():
            entity_ids_pair = pair_key.split("|")
            if len(entity_ids_pair) != 2:
                continue
            
            entity1_id, entity2_id = entity_ids_pair
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“
            for rel in relations:
                if self._is_relation_indicating_same_entity(rel['content']):
                    # æ‰¾åˆ°è¡¨ç¤ºåŒä¸€å®ä½“çš„å…³ç³»ï¼Œå‡†å¤‡åˆå¹¶
                    # é€‰æ‹©ç‰ˆæœ¬æ•°å¤šçš„ä½œä¸ºtarget
                    entity1_version_count = version_counts.get(entity1_id, 0)
                    entity2_version_count = version_counts.get(entity2_id, 0)
                    
                    if entity1_version_count >= entity2_version_count:
                        target_id = entity1_id
                        source_id = entity2_id
                    else:
                        target_id = entity2_id
                        source_id = entity1_id
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¢«åˆå¹¶
                    if source_id not in merged_entity_ids and target_id not in merged_entity_ids:
                        entities_to_merge_from_relations.append({
                            'target_id': target_id,
                            'source_id': source_id,
                            'relation_id': rel['relation_id'],
                            'relation_content': rel['content']
                        })
                    
                    break  # åªè¦æœ‰ä¸€ä¸ªå…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“ï¼Œå°±åˆå¹¶
        
        # æ‰§è¡Œä»å…³ç³»åˆ¤æ–­å‡ºçš„åˆå¹¶
        if entities_to_merge_from_relations:
            if verbose:
                print(f"    å‘ç° {len(entities_to_merge_from_relations)} å¯¹å®ä½“é€šè¿‡å…³ç³»åˆ¤æ–­ä¸ºåŒä¸€å®ä½“ï¼Œç›´æ¥åˆå¹¶")
            
            for merge_info in entities_to_merge_from_relations:
                target_id = merge_info['target_id']
                source_id = merge_info['source_id']
                relation_content = merge_info['relation_content']
                
                # æ‰§è¡Œåˆå¹¶
                merge_result = self.storage.merge_entity_ids(target_id, [source_id])
                merge_result["reason"] = f"å…³ç³»è¡¨ç¤ºåŒä¸€å®ä½“: {relation_content}"
                
                if verbose:
                    target_name = next((e.get('name', '') for e in entities_info if e.get('entity_id') == target_id), target_id)
                    source_name = next((e.get('name', '') for e in entities_info if e.get('entity_id') == source_id), source_id)
                    print(f"      åˆå¹¶å®ä½“ï¼ˆåŸºäºå…³ç³»ï¼‰: {target_name} ({target_id}) <- {source_name} ({source_id})")
                    print(f"        åŸå› : {relation_content}")
                
                # å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
                self._handle_self_referential_relations_after_merge(target_id, verbose)
                
                # è®°å½•å·²åˆå¹¶çš„å®ä½“å’Œåˆå¹¶æ˜ å°„
                merged_entity_ids.add(source_id)
                merge_mapping[source_id] = target_id
                
                # æ›´æ–°ç»“æœç»Ÿè®¡
                result["merge_details"].append(merge_result)
                result["entities_merged"] += merge_result.get("entities_updated", 0)
        
        # è¿‡æ»¤æ‰å·²é€šè¿‡å…³ç³»åˆå¹¶çš„å®ä½“å¯¹ï¼Œåªä¿ç•™éåŒä¸€å®ä½“çš„å…³ç³»
        filtered_existing_relations = {}
        for pair_key, relations in existing_relations_between.items():
            entity_ids_pair = pair_key.split("|")
            if len(entity_ids_pair) != 2:
                continue
            
            entity1_id, entity2_id = entity_ids_pair
            
            # å¦‚æœè¿™å¯¹å®ä½“å·²ç»é€šè¿‡å…³ç³»åˆå¹¶äº†ï¼Œè·³è¿‡
            if (entity1_id in merged_entity_ids and merge_mapping.get(entity1_id) == entity2_id) or \
               (entity2_id in merged_entity_ids and merge_mapping.get(entity2_id) == entity1_id):
                continue
            
            # è¿‡æ»¤æ‰è¡¨ç¤ºåŒä¸€å®ä½“çš„å…³ç³»
            filtered_relations = [
                rel for rel in relations 
                if not self._is_relation_indicating_same_entity(rel['content'])
            ]
            
            if filtered_relations:
                filtered_existing_relations[pair_key] = filtered_relations
        
        return filtered_existing_relations
    
    def _handle_self_referential_relations_after_merge(self, target_entity_id: str, verbose: bool = True) -> int:
        """
        å¤„ç†åˆå¹¶åäº§ç”Ÿçš„è‡ªæŒ‡å‘å…³ç³»
        
        åˆå¹¶æ“ä½œä¼šå°†æºå®ä½“çš„entity_idæ›´æ–°ä¸ºç›®æ ‡å®ä½“çš„entity_idï¼Œè¿™å¯èƒ½å¯¼è‡´åŸæœ¬ä¸æ˜¯è‡ªæŒ‡å‘çš„å…³ç³»å˜æˆè‡ªæŒ‡å‘å…³ç³»ã€‚
        ä¾‹å¦‚ï¼šå®ä½“A(ent_001)å’Œå®ä½“B(ent_002)ä¹‹é—´æœ‰å…³ç³»ï¼Œåˆå¹¶åBçš„entity_idå˜ä¸ºent_001ï¼Œè¿™ä¸ªå…³ç³»å°±å˜æˆäº†è‡ªæŒ‡å‘å…³ç³»ã€‚
        
        æ­¤æ–¹æ³•ä¼šï¼š
        1. æ£€æŸ¥ç›®æ ‡å®ä½“æ˜¯å¦æœ‰è‡ªæŒ‡å‘å…³ç³»
        2. å¦‚æœæœ‰ï¼Œå°†è¿™äº›å…³ç³»çš„å†…å®¹æ€»ç»“åˆ°å®ä½“çš„contentä¸­
        3. åˆ é™¤è¿™äº›è‡ªæŒ‡å‘å…³ç³»
        
        Args:
            target_entity_id: åˆå¹¶åçš„ç›®æ ‡å®ä½“ID
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            å¤„ç†çš„è‡ªæŒ‡å‘å…³ç³»æ•°é‡
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªæŒ‡å‘å…³ç³»
        self_ref_relations = self.storage.get_self_referential_relations_for_entity(target_entity_id)
        
        if not self_ref_relations:
            return 0
        
        if verbose:
            print(f"        æ£€æµ‹åˆ°åˆå¹¶åäº§ç”Ÿ {len(self_ref_relations)} ä¸ªè‡ªæŒ‡å‘å…³ç³»ï¼Œæ­£åœ¨å¤„ç†...")
        
        # è·å–å®ä½“çš„æœ€æ–°ç‰ˆæœ¬
        entity = self.storage.get_entity_by_id(target_entity_id)
        if not entity:
            if verbose:
                print(f"        è­¦å‘Šï¼šæ— æ³•è·å–å®ä½“ {target_entity_id}")
            return 0
        
        # æ”¶é›†æ‰€æœ‰è‡ªæŒ‡å‘å…³ç³»çš„content
        self_ref_contents = [rel['content'] for rel in self_ref_relations if rel.get('content')]
        
        if self_ref_contents:
            # ç”¨LLMæ€»ç»“è¿™äº›å…³ç³»å†…å®¹åˆ°å®ä½“çš„contentä¸­
            summarized_content = self.llm_client.merge_entity_content(
                old_content=entity.content,
                new_content="\n\n".join([f"å±æ€§ä¿¡æ¯ï¼š{content}" for content in self_ref_contents])
            )
            
            # åˆ›å»ºå®ä½“çš„æ–°ç‰ˆæœ¬
            new_entity_id = f"entity_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            new_entity = Entity(
                id=new_entity_id,
                entity_id=entity.entity_id,
                name=entity.name,
                content=summarized_content,
                physical_time=datetime.now(),
                memory_cache_id=entity.memory_cache_id
            )
            self.storage.save_entity(new_entity)
            
            if verbose:
                print(f"        å·²å°† {len(self_ref_contents)} ä¸ªè‡ªæŒ‡å‘å…³ç³»çš„å†…å®¹æ€»ç»“åˆ°å®ä½“contentä¸­")
        
        # åˆ é™¤è¿™äº›è‡ªæŒ‡å‘å…³ç³»
        deleted_count = self.storage.delete_self_referential_relations_for_entity(target_entity_id)
        
        if verbose:
            print(f"        å·²åˆ é™¤ {deleted_count} ä¸ªè‡ªæŒ‡å‘å…³ç³»")
        
        return deleted_count
    
    def _process_single_alias_relation(self, rel_info: Dict, verbose: bool = True) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªåˆ«åå…³ç³»ï¼ˆå¯å¹¶è¡Œè°ƒç”¨ï¼‰
        
        Args:
            rel_info: å…³ç³»ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
                - entity1_id, entity2_id: åŸå§‹entity_id
                - actual_entity1_id, actual_entity2_id: å®é™…ä½¿ç”¨çš„entity_idï¼ˆå¯èƒ½å·²åˆå¹¶ï¼‰
                - entity1_name, entity2_name: å®ä½“åç§°
                - content: åˆæ­¥çš„å…³ç³»contentï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºåˆæ­¥åˆ¤æ–­ï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - entity1_id, entity2_id: å®ä½“ID
                - entity1_name, entity2_name: å®ä½“åç§°
                - content: å…³ç³»content
                - relation_id: å…³ç³»ID
                - is_new: æ˜¯å¦æ–°åˆ›å»º
                - is_updated: æ˜¯å¦æ›´æ–°
            å¦‚æœå¤„ç†å¤±è´¥æˆ–è·³è¿‡ï¼Œè¿”å›None
        """
        actual_entity1_id = rel_info["actual_entity1_id"]
        actual_entity2_id = rel_info["actual_entity2_id"]
        entity1_name = rel_info["entity1_name"]
        entity2_name = rel_info["entity2_name"]
        preliminary_content = rel_info.get("content")  # åˆæ­¥çš„contentï¼ˆä»åˆ†æé˜¶æ®µç”Ÿæˆï¼‰
        
        if verbose:
            print(f"      å¤„ç†å…³ç³»: {entity1_name} -> {entity2_name}")
        
        try:
            # è·å–ä¸¤ä¸ªå®ä½“çš„å®Œæ•´ä¿¡æ¯
            entity1 = self.storage.get_entity_by_id(actual_entity1_id)
            entity2 = self.storage.get_entity_by_id(actual_entity2_id)
            
            if not entity1 or not entity2:
                if verbose:
                    print(f"        é”™è¯¯ï¼šæ— æ³•è·å–å®ä½“ä¿¡æ¯")
                return None
            
            # æ­¥éª¤0ï¼šå¦‚æœæœ‰åˆæ­¥contentï¼Œå…ˆç”¨å®ƒåˆ¤æ–­å…³ç³»æ˜¯å¦å­˜åœ¨å’Œæ˜¯å¦éœ€è¦æ›´æ–°
            if preliminary_content:
                if verbose:
                    print(f"        ä½¿ç”¨åˆæ­¥contentè¿›è¡Œé¢„åˆ¤æ–­: {preliminary_content[:100]}...")
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å…³ç³»
                existing_relations_before = self.storage.get_relations_by_entities(
                    actual_entity1_id,
                    actual_entity2_id
                )
                
                if existing_relations_before:
                    # æŒ‰relation_idåˆ†ç»„ï¼Œæ¯ä¸ªrelation_idåªä¿ç•™æœ€æ–°ç‰ˆæœ¬
                    relation_dict = {}
                    for rel in existing_relations_before:
                        if rel.relation_id not in relation_dict:
                            relation_dict[rel.relation_id] = rel
                        else:
                            if rel.physical_time > relation_dict[rel.relation_id].physical_time:
                                relation_dict[rel.relation_id] = rel
                    
                    unique_relations = list(relation_dict.values())
                    existing_relations_info = [
                        {
                            'relation_id': r.relation_id,
                            'content': r.content
                        }
                        for r in unique_relations
                    ]
                    
                    # æ„å»ºåˆæ­¥çš„extracted_relationæ ¼å¼
                    preliminary_extracted_relation = {
                        "entity1_name": entity1.name,
                        "entity2_name": entity2.name,
                        "content": preliminary_content
                    }
                    
                    # ç”¨LLMåˆ¤æ–­æ˜¯å¦åŒ¹é…å·²æœ‰å…³ç³»
                    match_result = self.llm_client.judge_relation_match(
                        preliminary_extracted_relation,
                        existing_relations_info
                    )
                    
                    if match_result and match_result.get('relation_id'):
                        # åŒ¹é…åˆ°å·²æœ‰å…³ç³»ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
                        relation_id = match_result['relation_id']
                        latest_relation = relation_dict.get(relation_id)
                        
                        if latest_relation:
                            need_update = self.llm_client.judge_content_need_update(
                                latest_relation.content,
                                preliminary_content
                            )
                            
                            if not need_update:
                                # ä¸éœ€è¦æ›´æ–°ï¼Œç›´æ¥è¿”å›ï¼Œè·³è¿‡åç»­è¯¦ç»†ç”Ÿæˆ
                                if verbose:
                                    print(f"        å…³ç³»å·²å­˜åœ¨ä¸”æ— éœ€æ›´æ–°ï¼ˆä½¿ç”¨åˆæ­¥contentåˆ¤æ–­ï¼‰ï¼Œè·³è¿‡è¯¦ç»†ç”Ÿæˆ: {relation_id}")
                                return {
                                    "entity1_id": actual_entity1_id,
                                    "entity2_id": actual_entity2_id,
                                    "entity1_name": entity1_name,
                                    "entity2_name": entity2_name,
                                    "content": latest_relation.content,
                                    "relation_id": relation_id,
                                    "is_new": False,
                                    "is_updated": False
                                }
                            else:
                                if verbose:
                                    print(f"        å…³ç³»å·²å­˜åœ¨ä½†éœ€è¦æ›´æ–°ï¼ˆä½¿ç”¨åˆæ­¥contentåˆ¤æ–­ï¼‰ï¼Œç»§ç»­ç”Ÿæˆè¯¦ç»†content: {relation_id}")
            
            # è·å–å®ä½“çš„memory_cacheï¼ˆåªæœ‰åœ¨éœ€è¦è¯¦ç»†ç”Ÿæˆæ—¶æ‰è·å–ï¼‰
            entity1_memory_cache = None
            entity2_memory_cache = None
            if entity1.memory_cache_id:
                from_cache = self.storage.load_memory_cache(entity1.memory_cache_id)
                if from_cache:
                    entity1_memory_cache = from_cache.content
            
            if entity2.memory_cache_id:
                to_cache = self.storage.load_memory_cache(entity2.memory_cache_id)
                if to_cache:
                    entity2_memory_cache = to_cache.content
            
            # æ­¥éª¤1ï¼šå…ˆåˆ¤æ–­æ˜¯å¦çœŸçš„éœ€è¦åˆ›å»ºå…³ç³»è¾¹ï¼ˆä½¿ç”¨å®Œæ•´çš„å®ä½“ä¿¡æ¯ï¼‰
            need_create_relation = self.llm_client.judge_need_create_relation(
                entity1_name=entity1.name,
                entity1_content=entity1.content,
                entity2_name=entity2.name,
                entity2_content=entity2.content,
                entity1_memory_cache=entity1_memory_cache,
                entity2_memory_cache=entity2_memory_cache
            )
            
            if not need_create_relation:
                if verbose:
                    print(f"        åˆ¤æ–­ç»“æœï¼šä¸¤ä¸ªå®ä½“ä¹‹é—´æ²¡æœ‰æ˜ç¡®çš„ã€æœ‰æ„ä¹‰çš„å…³è”ï¼Œè·³è¿‡åˆ›å»ºå…³ç³»è¾¹")
                return None
            
            if verbose:
                print(f"        åˆ¤æ–­ç»“æœï¼šä¸¤ä¸ªå®ä½“ä¹‹é—´å­˜åœ¨æ˜ç¡®çš„ã€æœ‰æ„ä¹‰çš„å…³è”ï¼Œéœ€è¦åˆ›å»ºå…³ç³»è¾¹")
            
            # æ­¥éª¤2ï¼šç”Ÿæˆå…³ç³»çš„memory_cacheï¼ˆä¸´æ—¶ï¼Œä¸ä¿å­˜ï¼‰
            relation_memory_cache_content = self.llm_client.generate_relation_memory_cache(
                [],  # å…³ç³»åˆ—è¡¨ä¸ºç©ºï¼Œå› ä¸ºè¿˜æ²¡æœ‰ç”Ÿæˆå…³ç³»content
                [
                    {"entity_id": actual_entity1_id, "name": entity1.name, "content": entity1.content},
                    {"entity_id": actual_entity2_id, "name": entity2.name, "content": entity2.content}
                ],
                {
                    actual_entity1_id: entity1_memory_cache or "",
                    actual_entity2_id: entity2_memory_cache or ""
                }
            )
            
            # æ­¥éª¤3ï¼šæ ¹æ®memory_cacheå’Œä¸¤ä¸ªå®ä½“ï¼Œç”Ÿæˆå…³ç³»çš„content
            relation_content = self.llm_client.generate_relation_content(
                entity1_name=entity1.name,
                entity1_content=entity1.content,
                entity2_name=entity2.name,
                entity2_content=entity2.content,
                relation_memory_cache=relation_memory_cache_content,
                preliminary_content=preliminary_content
            )
            
            if verbose:
                print(f"        ç”Ÿæˆå…³ç³»content: {relation_content}")
            
            # æ­¥éª¤4ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å…³ç³»
            existing_relations_before = self.storage.get_relations_by_entities(
                actual_entity1_id,
                actual_entity2_id
            )
            
            # æ„å»ºextracted_relationæ ¼å¼ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
            extracted_relation = {
                "entity1_name": entity1.name,
                "entity2_name": entity2.name,
                "content": relation_content
            }
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºæˆ–æ›´æ–°å…³ç³»
            need_create_or_update = False
            is_new_relation = False
            is_updated = False
            relation = None
            
            if not existing_relations_before:
                # 4a. å¦‚æœä¸å­˜åœ¨å…³ç³»ï¼Œéœ€è¦åˆ›å»ºæ–°å…³ç³»
                need_create_or_update = True
                is_new_relation = True
                if verbose:
                    print(f"        ä¸å­˜åœ¨å…³ç³»ï¼Œéœ€è¦åˆ›å»ºæ–°å…³ç³»")
            else:
                # 4b. å¦‚æœå­˜åœ¨å…³ç³»ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
                # æŒ‰relation_idåˆ†ç»„ï¼Œæ¯ä¸ªrelation_idåªä¿ç•™æœ€æ–°ç‰ˆæœ¬
                relation_dict = {}
                for rel in existing_relations_before:
                    if rel.relation_id not in relation_dict:
                        relation_dict[rel.relation_id] = rel
                    else:
                        if rel.physical_time > relation_dict[rel.relation_id].physical_time:
                            relation_dict[rel.relation_id] = rel
                
                unique_relations = list(relation_dict.values())
                existing_relations_info = [
                    {
                        'relation_id': r.relation_id,
                        'content': r.content
                    }
                    for r in unique_relations
                ]
                
                # ç”¨LLMåˆ¤æ–­æ˜¯å¦åŒ¹é…å·²æœ‰å…³ç³»
                match_result = self.llm_client.judge_relation_match(
                    extracted_relation,
                    existing_relations_info
                )
                
                if match_result and match_result.get('relation_id'):
                    # åŒ¹é…åˆ°å·²æœ‰å…³ç³»ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
                    relation_id = match_result['relation_id']
                    latest_relation = relation_dict.get(relation_id)
                    
                    if latest_relation:
                        need_update = self.llm_client.judge_content_need_update(
                            latest_relation.content,
                            relation_content
                        )
                        
                        if need_update:
                            # éœ€è¦æ›´æ–°
                            need_create_or_update = True
                            is_updated = True
                            if verbose:
                                print(f"        å…³ç³»å·²å­˜åœ¨ï¼Œéœ€è¦æ›´æ–°: {relation_id}")
                        else:
                            # ä¸éœ€è¦æ›´æ–°
                            if verbose:
                                print(f"        å…³ç³»å·²å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°: {relation_id}")
                            relation = latest_relation
                    else:
                        # æ‰¾ä¸åˆ°åŒ¹é…çš„å…³ç³»ï¼Œåˆ›å»ºæ–°å…³ç³»
                        need_create_or_update = True
                        is_new_relation = True
                        if verbose:
                            print(f"        æœªæ‰¾åˆ°åŒ¹é…çš„å…³ç³»ï¼Œåˆ›å»ºæ–°å…³ç³»")
                else:
                    # æ²¡æœ‰åŒ¹é…åˆ°å·²æœ‰å…³ç³»ï¼Œåˆ›å»ºæ–°å…³ç³»
                    need_create_or_update = True
                    is_new_relation = True
                    if verbose:
                        print(f"        æœªåŒ¹é…åˆ°å·²æœ‰å…³ç³»ï¼Œåˆ›å»ºæ–°å…³ç³»")
            
            # åªæœ‰åœ¨éœ€è¦åˆ›å»ºæˆ–æ›´æ–°æ—¶ï¼Œæ‰ä¿å­˜memory_cacheå¹¶åˆ›å»º/æ›´æ–°å…³ç³»
            if need_create_or_update:
                # ç”Ÿæˆæ€»ç»“çš„memory_cacheï¼ˆç”¨äºjsonçš„textå­—æ®µï¼‰
                cache_text_content = f"""å®ä½“1:
- name: {entity1.name}
- content: {entity1.content}
- memory_cache: {entity1_memory_cache if entity1_memory_cache else 'æ— '}

å®ä½“2:
- name: {entity2.name}
- content: {entity2.content}
- memory_cache: {entity2_memory_cache if entity2_memory_cache else 'æ— '}
"""
                
                # ä¿å­˜memory_cacheï¼ˆmdå’Œjsonï¼‰
                # ä»å®ä½“ä¸­è·å–æ–‡æ¡£åï¼ˆå¦‚æœå®ä½“æœ‰doc_nameï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå®ä½“çš„doc_nameï¼‰
                doc_name_from_entity = entity1.doc_name if hasattr(entity1, 'doc_name') and entity1.doc_name else ""
                
                relation_memory_cache = MemoryCache(
                    id=f"cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}",
                    content=relation_memory_cache_content,
                    physical_time=datetime.now(),
                    doc_name=doc_name_from_entity,
                    activity_type="çŸ¥è¯†å›¾è°±æ•´ç†-å…³ç³»ç”Ÿæˆ"
                )
                # ä¿å­˜memory_cacheï¼Œjsonçš„textæ˜¯ä¸¤ä¸ªå®ä½“çš„name+content+memory_cache
                self.storage.save_memory_cache(relation_memory_cache, text=cache_text_content)
                
                if verbose:
                    print(f"        ä¿å­˜å…³ç³»memory_cache: {relation_memory_cache.id}")
                
                # ä½¿ç”¨ä¿å­˜çš„memory_cache_idå¤„ç†å…³ç³»
                relation = self.relation_processor._process_single_relation(
                    extracted_relation,
                    actual_entity1_id,
                    actual_entity2_id,
                    relation_memory_cache.id,
                    entity1.name,
                    entity2.name,
                    verbose_relation=verbose,  # ä¼ é€’verboseå‚æ•°æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå…³ç³»æ“ä½œè¯¦æƒ…
                    doc_name=doc_name_from_entity
                )
            
            if relation:
                # è¿”å›å…³ç³»ä¿¡æ¯ï¼ˆç”¨äºåç»­ç»Ÿè®¡ï¼‰
                alias_detail = {
                    "entity1_id": actual_entity1_id,
                    "entity2_id": actual_entity2_id,
                    "entity1_name": entity1.name,
                    "entity2_name": entity2.name,
                    "content": relation_content,
                    "relation_id": relation.relation_id,
                    "is_new": is_new_relation,
                    "is_updated": is_updated
                }
                
                if is_new_relation:
                    if verbose:
                        print(f"        æˆåŠŸåˆ›å»ºæ–°å…³ç³»: {relation.relation_id}")
                elif is_updated:
                    if verbose:
                        print(f"        å…³ç³»å·²å­˜åœ¨ï¼Œå·²æ›´æ–°: {relation.relation_id}")
                else:
                    if verbose:
                        print(f"        å…³ç³»å·²å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°: {relation.relation_id}")
                
                return alias_detail
            else:
                if verbose:
                    print(f"        åˆ›å»ºå…³ç³»å¤±è´¥")
                return None
                    
        except Exception as e:
            if verbose:
                print(f"        å¤„ç†å¤±è´¥: {e}")
            import traceback
            if verbose:
                traceback.print_exc()
            return None
    
    def _finalize_consolidation(self, result: Dict, all_analyzed_entities_text: List[str], verbose: bool = True):
        """
        å®ŒæˆçŸ¥è¯†å›¾è°±æ•´ç†çš„æ”¶å°¾å·¥ä½œï¼ˆåˆ›å»ºæ€»ç»“è®°å¿†ç¼“å­˜ï¼‰
        
        Args:
            result: æ•´ç†ç»“æœå­—å…¸
            all_analyzed_entities_text: æ‰€æœ‰åˆ†æè¿‡çš„å®ä½“æ–‡æœ¬åˆ—è¡¨
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        # æ­¥éª¤5ï¼šåˆ›å»ºæ•´ç†æ€»ç»“è®°å¿†ç¼“å­˜
        if verbose:
            print(f"\næ­¥éª¤5: åˆ›å»ºæ•´ç†æ€»ç»“è®°å¿†ç¼“å­˜...")
        
        consolidation_summary = self.llm_client.generate_consolidation_summary(
            result["merge_details"],
            result["alias_details"],
            result["entities_analyzed"]
        )
        
        # æ„å»ºæ•´ç†ç»“æœæ‘˜è¦æ–‡æœ¬ï¼ˆç”¨äºä¿å­˜åˆ°JSONçš„textå­—æ®µï¼‰
        # åŒ…å«æ‰€æœ‰åˆ†æè¿‡çš„å®ä½“åˆ—è¡¨ + æ•´ç†ç»“æœæ‘˜è¦
        consolidation_text = f"""çŸ¥è¯†å›¾è°±æ•´ç†å®Œæˆ

æ•´ç†ç»“æœæ‘˜è¦ï¼š
- åˆ†æçš„å®ä½“æ•°: {result['entities_analyzed']}
- åˆå¹¶çš„å®ä½“è®°å½•æ•°: {result['entities_merged']}
- åˆ›å»ºçš„å…³è”å…³ç³»æ•°: {result['alias_relations_created']}

åˆå¹¶è¯¦æƒ…:
"""
        for merge_detail in result.get("merge_details", []):
            target_name = merge_detail.get("target_name", "æœªçŸ¥")
            source_names = merge_detail.get("source_names", [])
            consolidation_text += f"  - {target_name} <- {', '.join(source_names)}\n"
        
        consolidation_text += "\nå…³è”å…³ç³»è¯¦æƒ…:\n"
        for alias_detail in result.get("alias_details", []):
            entity1_name = alias_detail.get("entity1_name", "æœªçŸ¥")
            entity2_name = alias_detail.get("entity2_name", "æœªçŸ¥")
            is_new = alias_detail.get("is_new", False)
            is_updated = alias_detail.get("is_updated", False)
            status = "æ–°å»º" if is_new else ("æ›´æ–°" if is_updated else "å·²å­˜åœ¨")
            consolidation_text += f"  - {entity1_name} -> {entity2_name} ({status})\n"
        
        # æ·»åŠ æ‰€æœ‰åˆ†æè¿‡çš„å®ä½“åˆ—è¡¨ä¿¡æ¯
        if all_analyzed_entities_text:
            consolidation_text += "\n\n" + "="*80
            consolidation_text += "\næ‰€æœ‰ä¼ å…¥LLMè¿›è¡Œåˆ¤æ–­çš„å®ä½“åˆ—è¡¨\n"
            consolidation_text += "="*80
            consolidation_text += "".join(all_analyzed_entities_text)
        
        # åˆ›å»ºæ€»ç»“æ€§çš„è®°å¿†ç¼“å­˜
        summary_cache = MemoryCache(
            id=f"cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}",
            content=f"""# çŸ¥è¯†å›¾è°±æ•´ç†æ€»ç»“

## æ•´ç†æ€»ç»“

{consolidation_summary}
""",
            physical_time=datetime.now(),
            doc_name="",  # çŸ¥è¯†å›¾è°±æ•´ç†æ€»ç»“ä¸å…³è”ç‰¹å®šæ–‡æ¡£
            activity_type="çŸ¥è¯†å›¾è°±æ•´ç†æ€»ç»“"
        )
        
        # ä¿å­˜æ€»ç»“è®°å¿†ç¼“å­˜
        self.storage.save_memory_cache(
            summary_cache, 
            text=consolidation_text
        )
        
        if verbose:
            print(f"  å·²åˆ›å»ºæ•´ç†æ€»ç»“è®°å¿†ç¼“å­˜: {summary_cache.id}")
    
    def _build_entity_list_summary(self, entities_for_analysis: List[Dict]) -> str:
        """
        æ„å»ºä¼ å…¥LLMçš„entityåˆ—è¡¨æ€»ç»“
        
        Args:
            entities_for_analysis: ä¼ å…¥LLMåˆ†æçš„å®ä½“åˆ—è¡¨
            
        Returns:
            Markdownæ ¼å¼çš„å®ä½“åˆ—è¡¨æ€»ç»“
        """
        summary_lines = []
        summary_lines.append(f"å…± {len(entities_for_analysis)} ä¸ªå®ä½“ï¼š\n")
        
        for idx, entity_info in enumerate(entities_for_analysis, 1):
            entity_id = entity_info.get("entity_id", "æœªçŸ¥")
            name = entity_info.get("name", "æœªçŸ¥")
            content = entity_info.get("content", "")
            version_count = entity_info.get("version_count", 0)
            
            # æˆªå–contentçš„å‰100å­—ç¬¦ä½œä¸ºæ‘˜è¦
            content_snippet = content[:100] + "..." if len(content) > 100 else content
            
            summary_lines.append(f"{idx}. **{name}** (entity_id: `{entity_id}`, ç‰ˆæœ¬æ•°: {version_count})")
            summary_lines.append(f"   - å†…å®¹æ‘˜è¦: {content_snippet}")
            summary_lines.append("")
        
        return "\n".join(summary_lines)
    
    def _build_entity_list_text(self, entities_for_analysis: List[Dict]) -> str:
        """
        æ„å»ºåŒ…å«å®Œæ•´entityä¿¡æ¯çš„æ–‡æœ¬ï¼ˆç”¨äºä¿å­˜åˆ°JSONçš„textå­—æ®µï¼‰
        
        Args:
            entities_for_analysis: ä¼ å…¥LLMåˆ†æçš„å®ä½“åˆ—è¡¨
            
        Returns:
            åŒ…å«å®Œæ•´å®ä½“ä¿¡æ¯çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬entity_id, name, contentç­‰ï¼‰
        """
        text_lines = []
        text_lines.append(f"çŸ¥è¯†å›¾è°±æ•´ç† - ä¼ å…¥LLMè¿›è¡Œåˆ¤æ–­çš„å®ä½“åˆ—è¡¨ï¼ˆå…± {len(entities_for_analysis)} ä¸ªå®ä½“ï¼‰\n")
        text_lines.append("=" * 80)
        text_lines.append("")
        
        for idx, entity_info in enumerate(entities_for_analysis, 1):
            entity_id = entity_info.get("entity_id", "æœªçŸ¥")
            name = entity_info.get("name", "æœªçŸ¥")
            content = entity_info.get("content", "")
            version_count = entity_info.get("version_count", 0)
            
            text_lines.append(f"{idx}. å®ä½“åç§°: {name}")
            text_lines.append(f"   entity_id: {entity_id}")
            text_lines.append(f"   ç‰ˆæœ¬æ•°: {version_count}")
            text_lines.append(f"   å®Œæ•´å†…å®¹:")
            text_lines.append(f"   {content}")
            text_lines.append("")
            text_lines.append("-" * 80)
            text_lines.append("")
        
        return "\n".join(text_lines)


def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    import sys
    
    # é…ç½®
    storage_path = "./tmg_storage"
    document_paths = sys.argv[1:] if len(sys.argv) > 1 else []
    
    if not document_paths:
        print("ç”¨æ³•: python -m Temporal_Memory_Graph.processor <æ–‡æ¡£è·¯å¾„1> [æ–‡æ¡£è·¯å¾„2] ...")
        print("ç¤ºä¾‹: python -m Temporal_Memory_Graph.processor doc1.txt doc2.txt")
        return
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = TemporalMemoryGraphProcessor(
        storage_path=storage_path,
        window_size=1000,
        overlap=200,
        # llm_api_key="your-api-key",  # å¦‚æœéœ€è¦ï¼Œå–æ¶ˆæ³¨é‡Šå¹¶å¡«å…¥
        # llm_model="gpt-4",
        # llm_base_url="https://api.openai.com/v1",  # å¯è‡ªå®šä¹‰LLM API URL
        # embedding_model_path="/path/to/local/model",  # æœ¬åœ°embeddingæ¨¡å‹è·¯å¾„
        # embedding_model_name="sentence-transformers/all-MiniLM-L6-v2",  # æˆ–ä½¿ç”¨HuggingFaceæ¨¡å‹
    )
    
    # å¤„ç†æ–‡æ¡£
    processor.process_documents(document_paths, verbose=True)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_statistics()
    print("\nå¤„ç†å®Œæˆï¼")
    print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")


if __name__ == "__main__":
    main()
